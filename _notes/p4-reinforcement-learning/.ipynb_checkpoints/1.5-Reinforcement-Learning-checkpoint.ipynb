{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Reinforcement Learning\n",
    "\n",
    "Aside: Reinforcement Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "Image(filename=\"images/rl-01.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### API\n",
    "API is kinda like a box.\n",
    "(img)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename=\"images/rl-02.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Planner\n",
    "    - Last time Charles talked about the Planner box.\n",
    "    - Transition fn T, reward function R\n",
    "    - e.g. value or policy iteration\n",
    "\n",
    "2. Learner (Reinforcement learning)\n",
    "    - Will see many transitions.\n",
    "\n",
    "3. Modeler\n",
    "\n",
    "4. Simulator\n",
    "\n",
    "### Ways of gluing these together:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename=\"images/rl-03.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "(img)\n",
    "Planner with a learner inside vs a learner that uses a planner inside \n",
    "\n",
    "e.g.\n",
    "- Backgammon-playing RL used a RL-based planner."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Three Approaches to RL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename=\"images/rl-04.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Policy search\n",
    "    - Policies maps states to actions\n",
    "    - Adv: Direct Use -> Learning quantity you directly need to use\n",
    "    - Disadv: Indirect Learning (function). Data doesn't tell you what action to choose (Temporal credit assignment problem)\n",
    "\n",
    "2. Value-function based \n",
    "    - Maps states to values\n",
    "    - Adv: Direct learning\n",
    "    - Disadv: Indirect use. Need to turn into policy. But it's an okay conversion with some types of value function (conversion) using argmax.\n",
    "    - Adv: Simple if you do it right. Can be powerful.\n",
    "\n",
    "3. Model-based RL\n",
    "    - Going from T,R to U: Value iteration to solve Bellman equations. Not nice to do but doable.\n",
    "    - Adv: Direct learning.\n",
    "    - Indirect use cause you have to do planning and optimising (translate)\n",
    "\n",
    "Focus on Value-function based approaches for now.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## A new kind of value function\n",
    "\n",
    "$$U(s)=R(s) + \\gamma \\max_a \\sum_{s'}T(s,a,s')U(s')$$\n",
    "- Long-term value of being in a state is the reward for arriving in that state + the discounted reward of the future. (To leave the state we're going to choose an action and take the expectation ...)\n",
    "\n",
    "$$\\pi(s) = \\text{arg}\\max_a \\sum_{s'} T(s,a,s')U(s')$$\n",
    "- Look at expected values: Iterate over all possible next states weightedby their probability of utility of landing in state.\n",
    "\n",
    "### **NEW value function** Q: \n",
    "$$Q(s,a) = R(s) + \\gamma \\sum_{s')T(s,a,s')\\max_{a'}Q(s',a')$$\n",
    "- Q cause Q is in the latter half of the alphabet and many other letters are taken\n",
    "- Value for arriving in S, leaving via a (landing in s' with T probability), proceeding optimally thereafter.\n",
    "\n",
    "**Using Q to define U and $\\pi$**\n",
    "- Observe U(s) returns a scalar, $\\pi$(s) returns an acition\n",
    "$$U(s) = \\max_a Q(s,a)$$\n",
    "$$\\pi(s) = \\text{arg}\\max_a Q(s,a)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Q-learning\n",
    "- Evaluating the Bellman equations from data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename=\"images/rl-05.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Estimating Q from transitions\n",
    "\n",
    "$$Q(s,a) = R(s) + \\gamma \\sum_{s'}T(s,a,s)$$\n",
    "- Don't have R or T (vs MDP have R and T). It's polynomial to do if we have R and T\n",
    "- A transition is <s,a,r,s'>.\n",
    "$$\\hat Q(s,a) \\leftarrow^{\\alpha_t} r + \\gamma \\max_{a'} \\hat Q(s',a')$$\n",
    "where $\\alpha$ is the learning rate. 0 is no learning, 1 is full learning. 0.5 is averaging the previous and the new.\n",
    "\n",
    "\n",
    "- Don't have sum over transitions but have max a' and estimate of Q in next state.\n",
    "- Notation: e.g. $V\\leftarrow^\\alpha X$ means $V \\leftarrow V + \\alpha(X-V) = (1-\\alpha)V + \\alpha X$. Moving alpha of the way from X to V.\n",
    "    - Converges to E(X)\n",
    "    - Believe things less over time\n",
    "    - Like an average\n",
    "    - Adding things up and computing a weighted average with weightn decaying over time\n",
    "- Computing average value you'd get if you follow the optimal policy after taking a particular action.\n",
    "$$\\hat Q(s,a) \\leftarrow^{\\alpha_t} r + \\gamma \\max_{a'} \\hat Q(s',a')$$\n",
    "- Which we'll hand-wave and ignore that the above line is a moving target to get\n",
    "$$=E[r+\\gamma \\max_{a'} \\hat Q(s',a')]$$\n",
    "- from linearity of expectation\n",
    "$$=R(s) + \\gamma E_{s'}[\\max_{a'} \\hat Q(s',a')]$$\n",
    "$$=R(s) + \\gamma \\sum_{s'}T(s,a,s')\\hat Q(s',a')$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Q-learning convergence\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename=\"images/rl-06.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "line 2: Update $\\hat Q$.\n",
    "Remarkable that it's one line of code.\n",
    "Important caveat, need to visit all states etc.\n",
    "\n",
    "Q-learning is actually a **family of algorithms**.\n",
    "Vary along following themes:\n",
    "- How initialise $\\hat Q$?\n",
    "- How decay $\\alpha_t$?\n",
    "- How choose actions?\n",
    "    - Bad ways of choosing actions\n",
    "        - Always choose $a_0$ -> Bad b/c doesn't visit all actions and doesn't learn anything.\n",
    "        - Choose randomly -> May have learned Q, but we don't use it. Don't take advantage of anything you learn.\n",
    "        - Use $\\hat Q$. \n",
    "            - Can be bad (**Greedy action selection**): Only don't a_0 all the time if you update Q and get worse than terrible. Kind of a **local min**.\n",
    "            - i.e. if you set up $\\hat Q$ that makes some local min look better than the optimal.\n",
    "        - random restarts -> start it over over and over again.\n",
    "            - Going to take an even longer time to get to an answer\n",
    "            - Might help us get unstuck. (In random optimisation, we did this so if we got stuck we could throw out everything and get unstuck.)\n",
    "    - Use simulated annealing-like approach \n",
    "        -> Take uphill steps but randomly take a downhill step. Mixture of choosing randomly and using $\\hat Q$. So it's a random action.\n",
    "        - Take a random action sometimes $\\hat \\pi (s) = \\text{arg}\\max_a \\hat Q(s,a)\\text{w. prob} 1-\\epsilon$, random action otherwise.\n",
    "        - Chance of exploring whole space and learning true Q if you're stuck.\n",
    "\n",
    "## $\\epsilon$-greedy exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "Image(filename=\"rl-07.png\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Decayed $\\epsilon$ -> Over time more greedy, less random.\n",
    "- $\\hat Q -> Q$ from standard Q-learning convergence result\n",
    "\n",
    "### Exploration-exploitation dilemma\n",
    "**Fundamental tradeoff in RL**\n",
    "- Exploration: Getting data you need so you learn\n",
    "- Exploitation: Using what you know\n",
    "- Tradeoff because there's only one agent acting in the world but there are two types of actions.\n",
    "- How modelling and planning interact with each other\n",
    "\n",
    "- **Optimism in the face of uncertainty** Can also do EE via initialising $\\hat Q$. \n",
    "    - A*\n",
    "\n",
    "- Other approaches to EE: some in the model-based setting are more powerful because you can keep track of what you've learned in the environment where you haven't (Transfer?). Alg can then explore what it doesn't know and exploit what it does know.\n",
    "\n",
    "## Summary\n",
    "- Learn to solve an MDP not knowing T or R, but having the able to interact with the environment <s,a,r,s'>\n",
    "- Q-learning family: converges, Q function\n",
    "- Exploration-expolitation: learn and use\n",
    "    - Optimisation in the face of uncertainty\n",
    "- Approaches to RL\n",
    "- Connection to planning\n",
    "\n",
    "Connection to function approximation: overfitting comes up in more generalised RL situations."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
