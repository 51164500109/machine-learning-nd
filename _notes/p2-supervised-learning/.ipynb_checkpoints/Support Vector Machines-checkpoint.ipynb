{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "(SVM 1)\n",
    "\n",
    "Drawing it in the middle gives a biggest 'demilitarised' zone.\n",
    "Intuition:\n",
    "* There might bu other minuses near the minunes we can see that we risk chopping off if a line gets too close to the current minuses.\n",
    "* This data is just a sample from the population. // NN algorithms. Lines very close to e.g. the pluses -> believing training data too much. Overfitting.\n",
    "* Middle line is **consistent with the data but commits least to it.**\n",
    "* Interesting because it's not a complex overfit. They're all just lines.\n",
    "\n",
    "Hyperplanes:\n",
    "$$y = w^Tx+b$$\n",
    "* y represents the classification label\n",
    "* w representns parameters for our plane\n",
    "* b moves it out of the origin\n",
    "\n",
    "Taking some new point, projecting it onto the line, looking at the value you get when you project it.\n",
    "\n",
    "Value is positive if you are in the class, negative if you're not.\n",
    "\n",
    "Decision boundary being as far away from the data as possible without being inconsistent with it.\n",
    "\n",
    "Hyperplane equation at the decision boundary (neither positive nor negative output) is $w^Tx + b = 0$. \n",
    "\n",
    "What are the equations of the grey lines?\n",
    "* We know labels are {-1, +1}. Line that brushes up against positive example: want it s.t. the output of the line is +1 on the first point that it encounters.\n",
    "* $w^Tx+b=1$ for top grey line. Similarly, $w^T+b=-1$ for bottom grey line.\n",
    "\n",
    "(img)\n",
    "\n",
    "Need to maximise distance between two grey lines. The lines are parallel to one another. Choose one point on each grey line such that the line between them is perpendicular to the parallel lines.\n",
    "\n",
    "* Point on positive line: $w^Tx_1+b=1$\n",
    "* Point on negative line: $w^Tx_2+b-1$\n",
    "* Subtract to get line $w^T(x_1-x_2)=2\n",
    "* Divide both sides by the length of w: \n",
    "$$\\frac{w_t}{||w||}(x_1-x_2)=\\frac{2}{||w||}$$\n",
    "\n",
    "LHS: $x_1-x_2$ is projected onto the normalised vector (unit length, some direction).  This is callled the **margin**.\n",
    "\n",
    "w represents a vector perpendicular to the line (eqn of a plane)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to maximise $\\frac{2}{||w||}$ while classifying everything correctly. Let's turn the condition into a mathematical expression.\n",
    "\n",
    "That is,\n",
    "$$y_i(w^Tx_i + b) \\geq 1 \\forall i$$.\n",
    "\n",
    "* Q: Why geq 1 as opposed to geq 0?\n",
    "\n",
    "* Solve equivalent problem (LHS):\n",
    "$$\\min \\frac{1}{2}||w||^2$$\n",
    "\n",
    "This is easier because it's a quadratic programming problem and people know how to solvo those in straightforward ways. They always have a unique solution.\n",
    "\n",
    "Transform into quadratic programming form:\n",
    "$$\\max W(\\alpha) = \\sum_i \\alpha_i - \\frac{1}{2}\\sum_{i,j}\\alpha_i\\alpha_j y_iy_jx_i^Tx_j$$\n",
    "s.t. $\\alpha_i \\geq 0, \\sum_i \\alpha_i y_i = 0$.\n",
    "\n",
    "Properties\n",
    "* Once you find $\\alpha$, you can recover w: $w=\\sum_i\\alpha_iy_ix_i$.\n",
    "* You can also recover b from having w.\n",
    "* It turns out that those $\\alpha_i$s are mostly zero. -> Only a few x-s matter. Cause some datapoints don't factor into (don't matter for) w. -> Can find all of support you need in some vectors with the non-zero $\\alpha_i$s. -> **machine that only needs a few support vectors**.\n",
    "* Which vectors matter (will be part of the support vectors)? (Those closer to the line)\n",
    "\n",
    "* Similarities to Nearest Neighbours cause only local points matter. Like KNN except you've already done the work to figure out which ones you need and which ones you can throw away. -> Like instance-based learning but it's not completely lazy. (?)\n",
    "\n",
    "Dot product of $x_i^Tx_j$ -> Length of the projection. Measure of similarity (of direction) -> If they point in opposite directions it'll be a negative, if orthogonal it'll be 0, if in same direction it'll be positive and bigger.\n",
    "* Eqn: Find all pairs of points, figure out which ones matter, and think about how they relate to one another wrt their output labels wrt how similar they are.\n",
    "\n",
    "## Supposing not linearly separable\n",
    "\n",
    "* If have **outlier or intruder**: Can tradeoff: Maximise margin Makes the minimal set of errors while maximising the margin if you were allowed to flip a few points from pos to neg or vv.\n",
    "* 'Linearly married': minuses in a ring around the pluses. **Transform datapoints**.\n",
    "    - e.g. $\\Phi(q) = <q_1^2, q_2^2, \\sqrt2 q_1q_2>$\n",
    "    - $\\Phi(x)^T\\Phi(y)$"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
