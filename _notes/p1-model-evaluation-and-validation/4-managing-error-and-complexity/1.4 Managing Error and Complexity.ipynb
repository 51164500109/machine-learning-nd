{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Causes of Error\n",
    "\n",
    "### Bias-variance dilemma and no. of features\n",
    "High bias: Pays little attention to data, oversimplified\n",
    "- High error on training set\n",
    "- Low r^2, large SSE\n",
    "High variance: Pays too much attention to data (does not generalise well), overfits.\n",
    "- Much higher error on test set than on training data\n",
    "\n",
    "E.g. \n",
    "- few features used (if you have access to lots more) -> high bias.\n",
    "- Carefully minimised SSE (used lots of features, tuned parameters) -> High variance\n",
    "\n",
    "Want min number of features (simplicity) to achieve good accuracy (goodness of fit)\n",
    "- Few features, large r^2, low SSE.\n",
    "\n",
    "[overfit regression](images/p1-4-1-1.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Curse of Dimensionality\n",
    "As the number of **features or dimensions grows**, the amount of **data** we need to **generalise accurately** grows **exponentially**.\n",
    "\n",
    "e.g. KNN: distance or similarity function that assumes veerything is equally relevant, you'll have to see a lot of data before it washes itself away.\n",
    "\n",
    "e.g. 10 points uniformly distributed across a line segment. Each point owns a uniform part of the line segment. (// KNN)\n",
    "-> Move from a line segment to a 2D space. Each `x` still represents 1/10th of the space, but now it represents a bigger space. The farthest point that the first `x` is representing has a much larger distance. \n",
    "-> Q: How to make it such that each `x` has the same farthest-point-'diameter'? -> many more `x`s, e.g. 100.\n",
    "\n",
    "Think of it as points covering a space. If you want to cover the same amount of hyperspace...\n",
    "More features -> more volume to fill."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Learning Curves\n",
    "\n",
    "A **Learning Curve** is: a graph that compares the performance of a model on training and testing data over a varying number of training instances.\n",
    "\n",
    "- Should generally see performance improve as the number of training points increases.\n",
    "\n",
    "- By separating training and testing sets and graphing performance on each separately, we can get a better idea of how well the model can generalize to unseen data.\n",
    "\n",
    "A learning curve allows us to **verify when a model has learned as much as it can about the data**. When this occurs, the performance on both training and testing sets plateau and there is a consistent gap between the two error rates.\n",
    "\n",
    "### Bias\n",
    "When the training and testing errors converge and are quite high this usually means \n",
    "-> the model is biased. \n",
    "- No matter how much data we feed it, the model cannot represent the underlying relationship and therefore has systematic high errors.\n",
    "\n",
    "### Variance\n",
    "When there is a large gap between the training and testing error this generally means \n",
    "-> the model suffers from high variance. \n",
    "- Unlike a biased model, models that suffer from variance generally require more data to improve. \n",
    "- We can also limit variance by simplifying the model to represent only the most important features of the data.\n",
    "\n",
    "## Ideal Learning Curve\n",
    "The ultimate goal for a model is one that **has good performance that generalizes well to unseen data**. In this case, both the **testing and training curves converge at similar values**. \n",
    "- The smaller the gap between the training and testing sets, the better our model generalizes. \n",
    "- The better the performance on the testing set, the better our model performs.\n",
    "\n",
    "## Model Complexity\n",
    "The visual technique of graphing performance is not limited to learning. With most models, we can change the complexity by changing the inputs or parameters.\n",
    "\n",
    "A **model complexity graph** looks at training and testing curves as the model's complexity varies. The most common trend is that **as a model increases (in complexity), bias will fall off and variance will rise.**\n",
    "\n",
    "Scikit-learn provides a tool for validation curves which can be used to monitor model complexity by varying the parameters of a model. We'll explore the specifics of how these parameters affect complexity in the next course on supervised learning.\n",
    "\n",
    "### Learning Curves and Model Complexity\n",
    "So what is the relationship between learning curves and model complexity?\n",
    "\n",
    "If we were to take the learning curves of the same machine learning algorithm with the same fixed set of data, but create several graphs at different levels of model complexity, all the learning curve graphs would fit together into a 3D model complexity graph.\n",
    "\n",
    "If we took the final testing and training errors for each model complexity and visualized them along the complexity of the model we would be able to see how well the model performs as the model complexity increases.\n",
    "\n",
    "## Practical use of Model Complexity\n",
    "Knowing that **we can identify issues with bias and variance by analyzing a model complexity graph**, we now have a visual tool to help identify ways to optimize our models.\n",
    "\n",
    "This will be one of the core tools we use in the upcoming project.\n",
    "\n",
    "In the final section, we will introduce cross validation and grid search, which will give us a concrete, systematic way of searching through different levels of complexity to find the optimal model that complexity and learning curves give us a holistic understanding of."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
