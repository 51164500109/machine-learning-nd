{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Minimal number of features it takes to capture trends in the data.\n",
    "- Select best features\n",
    "- Add new features\n",
    "\n",
    "**Process**\n",
    "- Use human intuition\n",
    "    - POIs send emails to each other at a higher rate\n",
    "- Code up new feature\n",
    "    - Int number of messages to this person from POI\n",
    "- Visualise\n",
    "    - Does the new feature give discriminating power between POIs?\n",
    "- Repeat\n",
    "    - Can we do better? E.g. scale featre by total number of messages to or from that person.\n",
    "\n",
    "Observe\n",
    "- Outliers\n",
    "- Mixture of labelled points: Are there chunks in your visualisation where there are only one category of labels? (e.g. if <20% of emails sent to POIs -> all not POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "###\n",
    "### in poiFlagEmail() below, write code that returns a boolean\n",
    "### indicating if a given email is from a POI\n",
    "###\n",
    "\n",
    "import sys\n",
    "import reader\n",
    "import poi_emails\n",
    "\n",
    "def getToFromStrings(f):\n",
    "    '''\n",
    "    The imported reader.py file contains functions that we've created to help\n",
    "    parse e-mails from the corpus. .getAddresses() reads in the opening lines\n",
    "    of an e-mail to find the To: From: and CC: strings, while the\n",
    "    .parseAddresses() line takes each string and extracts the e-mail addresses\n",
    "    as a list.\n",
    "    '''\n",
    "    f.seek(0)\n",
    "    to_string, from_string, cc_string   = reader.getAddresses(f)\n",
    "    to_emails   = reader.parseAddresses( to_string )\n",
    "    from_emails = reader.parseAddresses( from_string )\n",
    "    cc_emails   = reader.parseAddresses( cc_string )\n",
    "\n",
    "    return to_emails, from_emails, cc_emails\n",
    "\n",
    "\n",
    "### POI flag an email\n",
    "\n",
    "def poiFlagEmail(f):\n",
    "    \"\"\" given an email file f,\n",
    "        return a trio of booleans for whether that email is\n",
    "        to, from, or cc'ing a poi \"\"\"\n",
    "\n",
    "    to_emails, from_emails, cc_emails = getToFromStrings(f)\n",
    "\n",
    "    ### poi_emails.poiEmails() returns a list of all POIs' email addresses.\n",
    "    poi_email_list = poi_emails.poiEmails()\n",
    "\n",
    "    to_poi = False\n",
    "    from_poi = False\n",
    "    cc_poi   = False\n",
    "\n",
    "    ### to_poi and cc_poi are related functions, which flag whether\n",
    "    ### the email under inspection is addressed to a POI, or if a POI is in cc\n",
    "    ### you don't have to change this code at all\n",
    "\n",
    "    ### there can be many \"to\" emails, but only one \"from\", so the\n",
    "    ### \"to\" processing needs to be a little more complicated\n",
    "    if to_emails:\n",
    "        ctr = 0\n",
    "        while not to_poi and ctr < len(to_emails):\n",
    "            if to_emails[ctr] in poi_email_list:\n",
    "                to_poi = True\n",
    "            ctr += 1\n",
    "    if cc_emails:\n",
    "        ctr = 0\n",
    "        while not to_poi and ctr < len(cc_emails):\n",
    "            if cc_emails[ctr] in poi_email_list:\n",
    "                cc_poi = True\n",
    "            ctr += 1\n",
    "\n",
    "\n",
    "    #################################\n",
    "    ######## your code below ########\n",
    "    ### set from_poi to True if #####\n",
    "    ### the email is from a POI #####\n",
    "    #################################\n",
    "\n",
    "    if from_emails:\n",
    "        ctr = 0\n",
    "        while not from_poi and ctr < len(from_emails):\n",
    "            if from_emails[ctr] in poi_email_list:\n",
    "                from_poi = True\n",
    "            ctr += 1\n",
    "    \n",
    "    \n",
    "\n",
    "    #################################\n",
    "    return to_poi, from_poi, cc_poi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beware of bugs - be skeptical of classifiers with near 100% accuracy\n",
    "\n",
    "When Katie was working on the Enron POI identifier, she engineered a feature that identified when a given person was on the same email as a POI. So for example, if Ken Lay and Katie Malone are both recipients of the same email message, then Katie Malone should have her \"shared receipt\" feature incremented. If she shares lots of emails with POIs, maybe she's a POI herself.\n",
    "\n",
    "Here's the problem: there was a subtle bug, that Ken Lay's \"shared receipt\" counter would also be incremented when this happens. And of course, then Ken Lay always shares receipt with a POI, because he is a POI. So the \"shared receipt\" feature became extremely powerful in finding POIs, because it effectively was encoding the label for each person as a feature.\n",
    "\n",
    "We found this first by being suspicious of a classifier that was always returning 100% accuracy. Then we removed features one at a time, and found that this feature was driving all the performance. Then, digging back through the feature code, we found the bug outlined above. We changed the code so that a person's \"shared receipt\" feature was only incremented if there was a different POI who received the email, reran the code, and tried again. The accuracy dropped to a more reasonable level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting rid of features\n",
    "Reasons\n",
    "- It's noisy\n",
    "- It causes overfitting\n",
    "- It is highly correlated with a feature that's already present\n",
    "- Additional features slow donw training/testing process\n",
    "\n",
    "## Features != Information.\n",
    "Features attempt to access information but are not info themselves. We want the info. // Quantity vs quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import cPickle\n",
    "import numpy\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(words_file = \"../tools/word_data.pkl\", authors_file=\"../tools/email_authors.pkl\"):\n",
    "    \"\"\" \n",
    "        this function takes a pre-made list of email texts (by default word_data.pkl)\n",
    "        and the corresponding authors (by default email_authors.pkl) and performs\n",
    "        a number of preprocessing steps:\n",
    "            -- splits into training/testing sets (10% testing)\n",
    "            -- vectorizes into tfidf matrix\n",
    "            -- selects/keeps most helpful features\n",
    "\n",
    "        after this, the feaures and labels are put into numpy arrays, which play nice with sklearn functions\n",
    "\n",
    "        4 objects are returned:\n",
    "            -- training/testing features\n",
    "            -- training/testing labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### the words (features) and authors (labels), already largely preprocessed\n",
    "    ### this preprocessing will be repeated in the text learning mini-project\n",
    "    authors_file_handler = open(authors_file, \"r\")\n",
    "    authors = pickle.load(authors_file_handler)\n",
    "    authors_file_handler.close()\n",
    "\n",
    "    words_file_handler = open(words_file, \"r\")\n",
    "    word_data = cPickle.load(words_file_handler)\n",
    "    words_file_handler.close()\n",
    "\n",
    "    ### test_size is the percentage of events assigned to the test set\n",
    "    ### (remainder go into training)\n",
    "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    ### text vectorization--go from strings to lists of numbers\n",
    "    # Some feature selection here  with (1) `stop_words=`english`' and\n",
    "    # (2) max_df -> don't include terms that have a document frequency \n",
    "    # strictly higher than the given thresholdts. \n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "    features_test_transformed  = vectorizer.transform(features_test)\n",
    "\n",
    "\n",
    "\n",
    "    ### feature selection, because text is super high dimensional and \n",
    "    ### can be really computationally chewy as a result\n",
    "    # Select best 10% of features using classifier\n",
    "    selector = SelectPercentile(f_classif, percentile=10)\n",
    "    selector.fit(features_train_transformed, labels_train)\n",
    "    features_train_transformed = selector.transform(features_train_transformed).toarray()\n",
    "    features_test_transformed  = selector.transform(features_test_transformed).toarray()\n",
    "\n",
    "    ### info on the data\n",
    "    print \"no. of Chris training emails:\", sum(labels_train)\n",
    "    print \"no. of Sara training emails:\", len(labels_train)-sum(labels_train)\n",
    "    \n",
    "    return features_train_transformed, features_test_transformed, labels_train, labels_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High dimensionality data -> many features\n",
    "\n",
    "## Bias-Variance Dilemma and Number of Features\n",
    "\n",
    "**High bias**: Pays little attention to data and is oversimplified\n",
    "- e.g. few features used\n",
    "- Low r^2, large SSE\n",
    "**High variance**: Pays too much attention to data, doesn't generalise well. Overfits.\n",
    "- e.g. carefully minimised SSE\n",
    "- Much higher error on test set than on training set\n",
    "\n",
    "Tradeoff between goodness of fit and the simplicity of fit.\n",
    "Want few features, low SSE, high r^2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regulatisation: Balancing error with no. of features\n",
    "- Method for automatically penalising extra features in your model\n",
    "Reverse-u plot (quality of model against no. of features)\n",
    "\n",
    "E.g. in regressions\n",
    "\n",
    "### Lasso Regression\n",
    "Minimise SSE + $\\lambda|\\beta|$, \n",
    "\n",
    "where $\\lambda$ is a penalty parameter and\n",
    "$\\beta$ is the coefficients of the regression (related to the number of features used)\n",
    "\n",
    "So gain of feature in minimising SSE has to outweigh the penalty of using that extra feature.\n",
    "\n",
    "$$y = \\sum m_ix_i + b$$\n",
    "\n",
    "**Process: **Lasso regression will try adding features one at a time. If it doesn't decrease SSE sufficiently, it won't add the feature. I.e. it sets the coefficients of those features to zero.\n",
    "\n",
    "Precisely, the **optimisation objective for Lasso is: ** $$(1 / (2 * \\text{n_samples})) * ||y - Xw||^2_2 + \\alpha * ||w||_1$$\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    from sklearn.linear_model import Lasso\n",
    "features, labels = GetMyData()\n",
    "regression = Lasso()\n",
    "regression.fit(features, labels)\n",
    "regression.predict([2,4])\n",
    "print(\"Coefficients: \", regression.coef_, \"\\nIntercept: \", regression.intercept_)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
