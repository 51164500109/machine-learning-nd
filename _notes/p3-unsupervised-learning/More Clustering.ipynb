{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Clustering\n",
    "\n",
    "Each algorithm is its own problem because there are many definitions of clustering.\n",
    "\n",
    "## 1. Single Linkage Clustering\n",
    "(Hierarchical agglomorative clustering. SLIC HAC)\n",
    "- Consider each object a cluster (n o bjects)\n",
    "- Define intercluster distance as the distance between the closest two points in the two clusters\n",
    "- Merge two closest clusters\n",
    "- Repeat n-k times to make n clusters\n",
    "\n",
    "Interesting: **median** distances. A non-metric statistic. Only ordering matters.\n",
    "\n",
    "### Characteristics\n",
    "- Deterministic\n",
    "- If doing in space where distances are equal to edge lengths on a graph, SLC is the same as a **minimum spanning tree**.\n",
    "- Running time O(n^3).\n",
    "    - Repeat k times (worst is n/2):\n",
    "        - Find two closest points O(n^2)\n",
    "        - Merge clusters together \n",
    "\n",
    "? Methods: Fibonacci heaps, hash tables.\n",
    "\n",
    "## 2. Soft Clustering\n",
    "- Allows for points to be shared -> Probabilitistically in certain clusters\n",
    "\n",
    "Assume the data was generated by\n",
    "1. Select one of K Gaussians (fixed known variance) uniformly\n",
    "2. Sample X_i from that Gaussian\n",
    "3. Repeat n times\n",
    "\n",
    "Task:\n",
    "Find a hypothesis h=<\\mu_1,...,\\mu_k> that maximises the probability of the data (ML -> maximum likelihood)\n",
    "\n",
    "ML mean of the Gaussian $\\mu$ is the mean of the data\n",
    "- Calculate mean of Gaussian by calculating sample mean\n",
    "\n",
    "What if there are k of them? -> Hidden Variables. \n",
    "$<x, z_1, z_2, ..., z_k>$ where $z_i$s indicate which cluster x is in.\n",
    "\n",
    "### **Expectation maximisation**\n",
    "$z_{ij}$ represents the likelihood element i comes from cluster j.\n",
    "Prop to p(el 1 was produced by cluster j).\n",
    "Pass that clustering info z to maximisation step\n",
    "Maximisation step: Compute means for clusters. if $z_{ij}$ is thought of as a {0,1} variable, it's like assigning elements to clusters. But because they are probabilities, we're soft assigning.\n",
    "\n",
    "\n",
    "- All points have some non-zero probability of being in each cluster.\n",
    "    - Makes sense because Gaussians have infinite extent\n",
    "\n",
    "#### Properties of EM\n",
    "- Monotonically non-decreasing likelihood\n",
    "    - i.e. generally goes in a good direction?\n",
    "- Does not converge (does in practice) (vs K Means does)\n",
    "- Will not diverge (bc working in probability space)\n",
    "- Can get stuck (Local optima problem) -> random restart\n",
    "- Works with any distribution (if E, M solvable). Usualy E (estimation) is harder. E-> probabilistic inference, Bayes stuff. M counting things.\n",
    "\n",
    "#### K-means arguments\n",
    "- Finite number of configurations\n",
    "    - Not getting worse w.r.t. error metric\n",
    "    -> As long as you have a way of breaking ties, you have to stop."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
