{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# PCA: Principal Component Analysis\n",
    "\n",
    "What is the dimensionality of data?\n",
    "- y = x is 1-dimensional. We can argue it is 1D even it has small deviations (think of those as noise).\n",
    "- But a cubic in PCA is 2D. (PCA only does shifts and rotations to create different coordinate systems. Probably does not include extra feature transformation)\n",
    "\n",
    "\n",
    "PCA: If you're given data of any shape, PCA finds a new coordinate system obtained from the old one by translation and rotation only and \n",
    "- it moves the centre of the coordinate system to the centre of the data.\n",
    "- it moves the x axis into the principal axis of variation relative to all other data points\n",
    "- it moves further axes orthogonal to the directions of variation\n",
    "\n",
    "So some data is 'PCA-ready', some is not (e.g. if it's a cubic). PCA can deal with vertical lines cause it's just vectors (vs regression uses functions).\n",
    "\n",
    "Questions\n",
    "- Is the data PCA-ready?\n",
    "- Does the major axis dominate? (Once you have spread captured in major axis, there's not much left in the minor axis(axes).\n",
    "    - e.g. circle -> no. both eigenvalues of same magnitude, haven't gained much by running PCA.\n",
    "\n",
    "## Measurable vs Latent Features\n",
    "\n",
    "Q: Given the features of a house, what is its price?\n",
    "\n",
    "Measurable variables\n",
    "- Square footage\n",
    "- No. of rooms\n",
    "- School ranking\n",
    "- Neighbourhood safety\n",
    "\n",
    "-> Probing **latent variables**\n",
    "- Size\n",
    "- Neighbourhood\n",
    "\n",
    "### Preserving information: How best to condense our measurable features to k features (where there are e.g. 2 latent variables)? \n",
    "\n",
    "- Feature selection tools\n",
    "    - Select k best (good if unknown no. of features)\n",
    "    - Select percentile\n",
    "\n",
    "Process:\n",
    "- Have many features, but I hypothesise a smaller number of features actually drive the patterns.\n",
    "- Try to make a **composite feature** (principal component) that more directly probes the underlying phenomenon.\n",
    "\n",
    "Tool for dimensionality reduction, also a good independent unsupervised learning tool.\n",
    "\n",
    "PC vs Regression:\n",
    "- Regression: Predicting\n",
    "- PC: Trying to find direction we can project our data onto to lose the least amount of info.\n",
    "\n",
    "## How to determine the principal component\n",
    "\n",
    "**Variance (stats)** : The spread of a data distribution (vs ML the willingness or flexibility of an alg to learn)\n",
    "\n",
    "**Principal component** of a dataset is the direction that has the **largest variance** because projecting onto this direction **retains the maximum amount of info in the original data**.\n",
    "\n",
    "(This is a compression algorithm)\n",
    "\n",
    "### Maximal variance and informal loss\n",
    "Information loss: perpendicular distance between point and line we're projecting the point onto.\n",
    "\n",
    "Projection onto direction of maximal variances minimises distance from old (higher-dimensional) point to its new transformed value -> Minimises information loss\n",
    "\n",
    "## PCA as a general algorithm for feature transformation\n",
    "- So far, separating or grouping features by hand (square footage, no. of rooms -> size). But this is not scalable.\n",
    "\n",
    "- Instead, put all features into PCA and ask PCA to pick first, second PCs. \n",
    "    - They'll likely be a mix of the intuitive latent variables, but it's a useful unsupervised learning technique.\n",
    "\n",
    "Max number of PCAs allowed by sklearn: min of no. of features and no. of training points\n",
    "\n",
    "\n",
    "## Working definition of PCA\n",
    "- PCA is a systematised way to transform input features into principal components\n",
    "- use principal components as new features\n",
    "- PCs are directions in data that maximise variance (min info loss) when you project or compress down onto them\n",
    "- The more variance of data along a PC, the hiher that PC is ranked.\n",
    "- Each PC is linearly independent with every other PC, so there is no overlap.\n",
    "- Max no. of PCs = min of  no. of input features and no. of training points."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.decomposition import PCA\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(data)\n",
    "\n",
    "# Print eigenvalues\n",
    "print(pca.explained_variance_ratio_)\n",
    "first_pc = pca.components_[0]\n",
    "socend_pc = pca.componentns_[1]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## When to use PCA\n",
    "- Figure out latent features driving the patterns in data\n",
    "- Dimensionality reduction\n",
    "    - Visualise high-dimensional data (scatterplot only have 2D available) -> Can visualise e.g. k means clustering\n",
    "    - Reduce noise (Hope 1st and 2nd PCs capture info and other minor ones capture noise)\n",
    "    - Preprocessing (reduce dim): Make other algs (regression, classification) work better b/c fewer inputs (e.g. Eigenfaces for facial identification -> feed into SVM)\n",
    "\n",
    "### PCA for Facial Recognition\n",
    "Good for PCA because\n",
    "- Pictures of faces generally have high input dimensionality (many pixels)\n",
    "- Faces have general patterns that could be captured in smaller number of dimensions (two eyes on top, moth/chin on bottom)\n",
    "\n",
    "### Selecting a number of PCs\n",
    "- Train on different number of PCs and choose optimal\n",
    "- Be v careful about throwing out features before you do PCA. Sometimes you might do it because PCA is computationally expensive, but be careful when you do."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Train-test split\n",
    "\n",
    "# from 1850 features to 150\n",
    "n_components = 150 \n",
    "\n",
    "# Extracting the top 150 faces from >1200 faces\n",
    "pca = RandomizedPCA(n_components=n_components, whiten=True).fit(X_train)\n",
    "\n",
    "eigenfaces = pca.components_.reshape((n_components, h, w))\n",
    "`\n",
    "# Transform into PCA representation\n",
    "# i.e. project input data on the eigenfaces orthonormal basis\n",
    "X_train_pca = pca.transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "\n",
    "# \n",
    "clf = GridSearchCV(...)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
