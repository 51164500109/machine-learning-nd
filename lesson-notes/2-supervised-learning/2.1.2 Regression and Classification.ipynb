{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Regression and Classification\n",
    "\n",
    "(Charles and Michael)\n",
    "\n",
    "**Supervised learning**: Take examples of inputs and outputs. Now, given a new input, predict its output.\n",
    "\n",
    "Regression is special because we're **mapping continuous inputs to outputs**. vs discrete input to discrete/continuous output.\n",
    "\n",
    "Origin: Height of children *regressing* to the mean.\n",
    "Term misused -> people were really referring to the idea of finding a mathematical relationship based on measurements of points.\n",
    "\n",
    "Similar with Reinforcement learning: mathmos named RL that but that's not what RL (first in psychology) actually is.\n",
    "\n",
    "[Origin of Regression](r-and-c-03.png)\n",
    "\n",
    "## Line of best fit\n",
    "How do we find the line of best fit? (Least squared error)\n",
    "- Calculus\n",
    "Loss error function chosen: Squares because it's well-behaved, **smooth**.\n",
    "\n",
    "$$E(c) = \\sum_{i=1}^n (y_i-c)^2 $$\n",
    "\n",
    "Differentiate with respect to c. $c = \\bar y$.\n",
    "\n",
    "Fitting polynomial functions.\n",
    "Parabola has more degrees of freedom. If the best fit was a line, the parabola wouldn't have any curve in it.\n",
    "-> Can't go past order = number of data points. (n-1?)\n",
    "\n",
    "[img](r-and-c-07.png)\n",
    "\n",
    "[Training Error with Degrees](r-and-c-07b.png)\n",
    "\n",
    "[Polynomial Regression](r-and-c-10.png)\n",
    "-> Solve through least squares:\n",
    "(X^T)X has inverse.\n",
    "\n",
    "['Solving' for polynomial regression](r-and-c-10b.png)\n",
    "\n",
    "Need to use least squares because of \n",
    "### Errors\n",
    "Not modelling f, but f + $\\epsilon$.\n",
    "\n",
    "Sources of error:\n",
    "- Sensor error (physical)\n",
    "- Misrepresented data\n",
    "- Data entry (transcription) error\n",
    "- Unmodeled influences\n",
    "\n",
    "Want to fit signal, not underlying error or noise.\n",
    "\n",
    "## Cross-validation\n",
    "\n",
    "If we don't like e.g. a linear model because we think there's too much error, we could use a higher order model.\n",
    "BUT then it wouldn't generalise well. Goal is to generalise.\n",
    "\n",
    "**Test set is just a stand-in for what we don't know what we're going to see in the real world.**\n",
    "\n",
    "Nothing we do on our training or test sets make sense unless we think they will be **representative** of what we'll see in the future in the real world.\n",
    "\n",
    "I.e. \n",
    "### **count on data being IID**: \n",
    "Independent and identically distributed (coming from the same source).\n",
    "-> Fundamental assumption in many algorithms.\n",
    "\n",
    "We want to use a model that is complex enough to fit the data without causing problems on the test set.\n",
    "\n",
    "### Cross-validation set\n",
    "We can take some of the training set and pretend it's a test set and it's not cheating because it's not actually the test set.\n",
    "\n",
    "Folds: (sheep?)\n",
    "[CV](r-and-c-13.png)\n",
    "\n",
    "Blue line initially higher because predicting on some data it hasn't seen before vs red line predicting on same data it was trained on.\n",
    "\n",
    "More power tends to overfit training data at expense of generalisation.\n",
    "[CV](r-and-c-14.png)\n",
    "\n",
    "[CV](r-and-c-14b.png)\n",
    "\n",
    "## Other Input Spaces\n",
    "So far have talked about \n",
    "- scalar input, continuous. x\n",
    "\n",
    "Also have:\n",
    "- vector iinput, continuous. x\n",
    "     - e.g. Features: size and distance from zoo for housing prices.\n",
    "     - Generalise to planes and hyperplanes vs lines.\n",
    "- discrete {0,1}, vector or scalar.\n",
    "    - e.g. predicting credit score features: Do they have a job? (discrete) What is the value of assets they currently hold? (continuous)\n",
    "    - Encoding features:\n",
    "        - Enumerating (red is 1, beige is 2, brown is 3. Implies beige is between red and brown, which it kinda isn't.)\n",
    "        - Boolean vectors for each\n",
    "\n",
    "## Summary\n",
    "- Historical facts\n",
    "- Model selection and under/over-fitting\n",
    "- Cross-validation\n",
    "- Linear, polynomial regression\n",
    "- Best constant in terms of squared error: Mean\n",
    "- (Input) Representation for regression"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
