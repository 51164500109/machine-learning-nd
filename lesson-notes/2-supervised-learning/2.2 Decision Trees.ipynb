{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Supervised Learning\n",
    "\n",
    "## Classification vs Regression\n",
    "\n",
    "Two types of supervised learning: Classification and Regression\n",
    "C: Taking some input and mapping it to some discrete label.\n",
    "R: More about continuous-valued functions. Mapping pictures of Michael to the length of his hair.\n",
    "- MAPPING TO discrete / continuous output.\n",
    "\n",
    "## Terminology\n",
    "\n",
    "- Instances: Input (Vectors, sets of). Can be credit score, pixels.\n",
    "- Concept: Function that maps inputs to outputs. (Like a concepts of 'what defines maleness')\n",
    "- Target concept: ANSWER. The specific function we're trying to find out of all the possible concepts.\n",
    "- Hypothesis: Class. The set of all possible concepts you're willing to entertain. Could be all possible functions but it might be hard to figure out which function is best given finite data.\n",
    "    - Already now our hypothesis class is restricted to classification.\n",
    "\n",
    "- Sample (Training set): Set of all input paired with output.\n",
    "- Candidate: A concept you think might be the target concept.\n",
    "- Testing set: Determine if candidate concept does a good job or not by testing it on the testing set. (Apply candidate concept on input and check predictions against labels.\n",
    "\n",
    "- Testing set needs to be different from the training set else it's cheating."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Decision Trees\n",
    "\n",
    "E.g. of dating and choosing whether or not to go into a certain restaurant.\n",
    "- Some features have to do with the restaurant and some have to do with things external to the restaurant (whether or not you're hungry)\n",
    "- Some irrelevant features (number of cars parked across the country)\n",
    "\n",
    "Consider the representation of a DT:\n",
    "- Ask a series of questions and depending on the answers move from the root of the tree (top) along different paths down the tree.\n",
    "- Leaves of the tree contain ANSWERs (output). Nodes have attributes (features).\n",
    "\n",
    "### Algorithm\n",
    "Thoughts:\n",
    "- 20 questions example. Think about the ordering of questions.\n",
    "- Goal in asking questions was to **further** narrow down possibilities as much as possible.\n",
    "- That is, the usefulness of each question depends on the answers you have to the previous questions.\n",
    "- DT vs 20 questions: with DT, can build entire flowchart at the start vs 20 questions asking interactively.\n",
    "\n",
    "Recipe:\n",
    "1. Pick the best attribute\n",
    "    - Best: splitting the data roughly in half (say)\n",
    "2. Asked question\n",
    "3. Follow the path of the answer\n",
    "4. Go to 1\n",
    "\n",
    "UNTIL got an answer.\n",
    "\n",
    "If an attribute node splits data into half but doesn't change distributions, it could arguably be bad because it doesn't help and only **contributes to overfitting**.\n",
    "\n",
    "### Decision Trees: Expressiveness\n",
    "e.g. Boolean A AND B.\n",
    "\n",
    "A -> F -> leaf; No\n",
    "\n",
    "    -> T\n",
    "        -> B ->\n",
    "            -> F -> leaf: No\n",
    "            -> T -> leaf: Yes\n",
    "\n",
    "The same if you switch A and B around.\n",
    "Cause A and B are commutative: The play the same role in the function.\n",
    "\n",
    "Also: OR, XOR (exclusive OR)\n",
    "- Representations of a truth table.\n",
    "\n",
    "### Size of DTs\n",
    "\n",
    "For AND and OR, need two nodes. For XOR need three nodes. Scaled,\n",
    "\n",
    "1. n-OR: If any of the n nodes is true, n-OR is true.\n",
    "    - n nodes. Size of DT is linear, O(n).\n",
    "\n",
    "2. n-XOR: Parity, e.g. pick odd parity. If the number of attributes that are true is odd, then True. Else False.\n",
    "    - 2^n - 1 nodes. Size of DT is exponential, O(2^n).\n",
    "    - Sub-trees are a version of XOR.\n",
    "\n",
    "-> Want to look at more **any** questions than **parity** questions.\n",
    "\n",
    "-> Can feature engineer to solve this. **The hardest problem is coming up with a good representation.**\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Exactly how expressive is a decision tree?\n",
    "- i.e. how many decision trees do we have to look at?\n",
    "- e.g. n boolean attributes and output is boolean.\n",
    "\n",
    "- Nodes: n!\n",
    "- Truth table: 2^n rows.\n",
    "    - How many ways are there to fill in the outputs? 2^n cells to fill, so 2^2^n.\n",
    "\n",
    "n = 6 -> 2^2^6 is of order of magnitude 10^19.\n",
    "- Decision trees are expressive.\n",
    "- Need a smart way to search all DTs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3: Alg\n",
    "\n",
    "Loop forever until solve problem:\n",
    "- A <- best attribute\n",
    "- Assign A as a decision attribute for NODE.\n",
    "- For each value of A, create a descendant of NODE\n",
    "- Sort training examples to leaves\n",
    "- If examples perfectly classified, STOP\n",
    "- Else iterate over leaves to find best attribute that will sort leaves\n",
    "\n",
    "### Finding the Best attribute: Information gain.\n",
    "Gain(S,A) = Entropy(S) - expected or avg entropy you'd have over each set of examples that you have with a particular value.\n",
    "\n",
    "$$\\max Gain(S,A) = Entropy(S) - \\sum_v \\frac{|S_v|}{|S|}Entropy(S_v)$$\n",
    "\n",
    "S is collection of training examples you're looking at\n",
    "A is the attribute\n",
    "\n",
    "**Info Gain: Reduction in randomness of data based on knowing value of attribute.**\n",
    "\n",
    "**Entropy: A measure of randomness.** \n",
    "- E.g. fair coin entropy is 1. -> No basis going into flipping the coin to guess if it's heads or tails.\n",
    "\n",
    "#### Formula for Entropy\n",
    "$$-\\sum_v p(v)logp(v)$$\n",
    "\n",
    "c.f. randomised optimisation later for more details.\n",
    "\n",
    "Previously we said we preferred splits that were less random (lower entropy). We want there to be info gain "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## ID3 Bias: Inductive Bias\n",
    "\n",
    "**Two kinds of biases we worry about when thinking about algorithms who search through space:**\n",
    "- Restriction Bias: Hypothesis set that you care about (e.g. all decision trees. Not consider quadratic equations...)\n",
    "- Preference Bias: What sorts of hypotheses from this hypothesis test that we prefer -> at the heart of inductive bias.\n",
    "\n",
    "Inductive bias of ID3 algorithm\n",
    "- Since making decision top-down, more likely to choose trees that have **good splits near the top** than not. Even if both represent the function that we care about.\n",
    "- **Correct over incorrect**: Prefers ones that model the data better to ones that model the data worse.\n",
    "- Prefers **shorter trees** to longer ones. Comes naturally from preference for good splits at the top.\n",
    "\n",
    "## DTs: Other Considerations\n",
    "1. What if we had **continuous attributes**?\n",
    "    - Use ranges or '<20?' splits, binary search.\n",
    "2. When do we stop?\n",
    "    - You might think 'when everything is classified correctly.' BUT if there's **noise** :(\n",
    "    - Or if we've run out of attributes (doesn't help when we have continuous attributes)\n",
    "    - No overfitting (overfit by having a tree that's too big, violates Occam's Razor.)\n",
    "        - CV?\n",
    "        - Stop expanding tree once you reach a certain accuracy on a validation set\n",
    "    - **Pruning** -> smaller tree. (vid 28)\n",
    "        - Need to have **votes on output**.\n",
    "3. Regression\n",
    "    - Q: What are the splitting criteria?\n",
    "        - Try to measure how mixed up things are using **variance**.\n",
    "    - What would you do with leaves? (Output) -> Average? Local linear fit?\n",
    "\n",
    "## Conclusion\n",
    "- Representation\n",
    "- ID3: A top-down learning algorithm\n",
    "- Expressiveness of DTs\n",
    "- Bias of ID3 (Inductive Bias)\n",
    "- 'Best attributes' (Deciding on splits) Maximum information gain\n",
    "- Dealing with overfitting e.g. using pruning.\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
