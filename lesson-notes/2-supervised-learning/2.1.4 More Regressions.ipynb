{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Using data to build a model that predicts a numerical output based on a set of numerical inputs.\n",
    "\n",
    "## 1. Parametric regression\n",
    "\n",
    "Building a model where we represent a model using a set of parameters.\n",
    "- e.g. polynomial regression\n",
    "- Biased in that you have a guess for what kind of equation the underlying model is -> Can be good if you do know.\n",
    "- Don't need to store original data so more space-efficient\n",
    "- Can't update the model as more data is gathered.\n",
    "- Training is slow, querying is fast.\n",
    "\n",
    "## 2. K Nearest Neighbour (KNN)\n",
    "\n",
    "**Instance-based methods:** Data-centric approach: Keep data and use it when we make a query. (Best for where you don't have a guess for what the underlying mathematical method might look like because instance-based methods can fit any shape.)\n",
    "**Take mean of k nearest neighbours' y-value.**\n",
    "Repeat across the x-axis\n",
    "- Interpolates smoothly around datapoints.\n",
    "- Unbiased: Avoid having to assume a particular model. Good for fitting complex datasets where we don't know what the underlying model is like.\n",
    "- Hard to apply with a large dataset (takes up a lot of memory)\n",
    "- New data can be added easily\n",
    "- Training is fast, querying is potentially slow.\n",
    "\n",
    "## 3. Kernel Regression\n",
    "\n",
    "Weigh each datapoint according to how far away they are vs KNN each neighbour gets essentially equal weight.\n",
    "\n",
    "## Numpy Polyfit\n",
    "\n",
    "numpy.polyfit(x, y, deg, rcond=None, full=False, w=None, cov=False)[source]\n",
    "Least squares polynomial fit.\n",
    "\n",
    "Fit a polynomial p(x) = p[0] * x**deg + ... + p[deg] of degree deg to points (x, y). Returns a vector of coefficients p that minimises the squared error.\n",
    "\n",
    "Parameters:\t\n",
    "x : array_like, shape (M,)\n",
    "x-coordinates of the M sample points (x[i], y[i]).\n",
    "y : array_like, shape (M,) or (M, K)\n",
    "y-coordinates of the sample points. Several data sets of sample points sharing the same x-coordinates can be fitted at once by passing in a 2D-array that contains one dataset per column.\n",
    "deg : int\n",
    "Degree of the fitting polynomial\n",
    "rcond : float, optional\n",
    "Relative condition number of the fit. Singular values smaller than this relative to the largest singular value will be ignored. The default value is len(x)*eps, where eps is the relative precision of the float type, about 2e-16 in most cases.\n",
    "full : bool, optional\n",
    "Switch determining nature of return value. When it is False (the default) just the coefficients are returned, when True diagnostic information from the singular value decomposition is also returned.\n",
    "w : array_like, shape (M,), optional\n",
    "weights to apply to the y-coordinates of the sample points.\n",
    "cov : bool, optional\n",
    "Return the estimate and the covariance matrix of the estimate If full is True, then cov is not returned.\n",
    "Returns:\t\n",
    "p : ndarray, shape (M,) or (M, K)\n",
    "Polynomial coefficients, highest power first. If y was 2-D, the coefficients for k-th data set are in p[:,k].\n",
    "residuals, rank, singular_values, rcond :\n",
    "Present only if full = True. Residuals of the least-squares fit, the effective rank of the scaled Vandermonde coefficient matrix, its singular values, and the specified value of rcond. For more details, see linalg.lstsq.\n",
    "V : ndarray, shape (M,M) or (M,M,K)\n",
    "Present only if full = False and cov`=True. The covariance matrix of the polynomial coefficient estimates. The diagonal of this matrix are the variance estimates for each coefficient. If y is a 2-D array, then the covariance matrix for the `k-th data set are in V[:,:,k]\n",
    "Warns:\t\n",
    "RankWarning\n",
    "The rank of the coefficient matrix in the least-squares fit is deficient. The warning is only raised if full = False.\n",
    "The warnings can be turned off by\n",
    ">>> import warnings\n",
    ">>> warnings.simplefilter('ignore', np.RankWarning)\n",
    "See also\n",
    "polyval\n",
    "Computes polynomial values.\n",
    "linalg.lstsq\n",
    "Computes a least-squares fit.\n",
    "scipy.interpolate.UnivariateSpline\n",
    "Computes spline fits.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This function takes in a list of regression values x and y and a degree, and outputs a polynomial in the form of a list p = [p[0],p[1],...,p[degree]] as in the model above.\n",
    "\n",
    "Another tool you may see or use in the future is the SciKit-Learn preprocessing function, PolynomialFeatures, which you can read about here. This function adds features to a dataset which are quadratic (or higher) combinations of the previous features.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\n",
    "# Regression and Classification programming exercises\n",
    "#\n",
    "#\n",
    "\n",
    "\n",
    "#\n",
    "#\tIn this exercise we will be taking a small data set and computing a linear function\n",
    "#\tthat fits it, by hand.\n",
    "#\t\n",
    "\n",
    "#\tthe data set\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sleep = [5,6,7,8,10]\n",
    "scores = [65,51,75,75,86]\n",
    "\n",
    "\n",
    "def compute_regression(sleep,scores):\n",
    "\n",
    "    #\tFirst, compute the average amount of each list\n",
    "\n",
    "    avg_sleep = np.mean(sleep)\n",
    "    avg_scores = np.mean(scores)\n",
    "\n",
    "    #\tThen normalize the lists by subtracting the mean value from each entry\n",
    "\n",
    "    normalized_sleep  = [s - avg_sleep for s in sleep]\n",
    "    normalized_scores = [s - avg_scores for s in scores]\n",
    "    print normalized_sleep\n",
    "    #\tCompute the slope of the line by taking the sum over each student\n",
    "    #\tof the product of their normalized sleep times their normalized test score.\n",
    "    #\tThen divide this by the sum of squares of the normalized sleep times.\n",
    "\n",
    "    \n",
    "    slope =  np.dot(normalized_sleep, normalized_scores) / np.dot(normalized_sleep, normalized_sleep)\n",
    "    #\tFinally, We have a linear function of the form\n",
    "    #\ty - avg_y = slope * ( x - avg_x )\n",
    "    #\tRewrite this function in the form\n",
    "    #\ty = m * x + b\n",
    "    #\tThen return the values m, b\n",
    "\n",
    "    m = slope\n",
    "    b = - slope * avg_sleep + avg_scores\n",
    "\n",
    "    print \"m, b = \", m, b\n",
    "    return m,b\n",
    "\n",
    "\n",
    "if __name__==\"__main__\":\n",
    "    m,b = compute_regression(sleep,scores)\n",
    "    print \"Your linear model is y={}*x+{}\".format(m,b)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#\n",
    "#\tPolynomial Regression\n",
    "#\n",
    "#\tIn this exercise we will examine more complex models of test grades as a function of \n",
    "#\tsleep using numpy.polyfit to determine a good relationship and incorporating more data.\n",
    "#\n",
    "#\n",
    "#   at the end, store the coefficients of the polynomial you found in coeffs\n",
    "#\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "sleep = [5,6,7,8,10,12,16]\n",
    "scores = [65,51,75,75,86,80,0]\n",
    "\n",
    "coeffs = np.polyfit(sleep, scores, 2)\n",
    "\n",
    "print coeffs"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
