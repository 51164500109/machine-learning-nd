{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bayesian Learning\n",
    "\n",
    "Thinking omore generally about learning theory\n",
    "\n",
    "Claim we're trying to **learn the best hypothesis we can given data and some domain knowledge**.\n",
    "\n",
    "Try to be more precise about 'best': **most probable** hypothesis (most probably the correct one). I.e. **$$\\text{argmax}_{h\\in H} P(h|D)$$**, where D is data.\n",
    "\n",
    "\n",
    "Bayes's Rules\n",
    "$$P(h|D) = \\frac{P(D|h)P(h)}{P(D)}$$\n",
    "\n",
    "Follows directly from the chain rule in probability. Numerator is probability of D and h together (conjunction).\n",
    "So $$Pr(a,b) = P(a|b)*P(b)$$.\n",
    "\n",
    "Ask what each term means: \n",
    "- **P(D)** is your prior belief for seeing a particular set of data.\n",
    "- **P(D|h)**: Likelihood we'll see some data where a hypothesis is true. Data is training data. Is set of inputs and labels corresponding to those inputs. $D = \\{(x_i, d_i)\\}$. **P(seeing labels $d_i$)** where h is true and we have inputs $x_i$.\n",
    "    - P(D|h) is a lot easier to compute.\n",
    "    - ? version space.\n",
    "    - Kind of like accuracy?\n",
    "- **Pr(h)**: Prior on hypothesis. Encapsulates belief that a hypothesis is likely or unlikely compared to other hypotheses. I.e. our **domain knowledge**.\n",
    "\n",
    "vs kernels and similarity functions for domain knowledge.\n",
    "\n",
    "**Priors matter**. E.g. if we only give the test to people who have certain symptoms (add evidence), you can increase the spleentitis prior. It's actually changing the posterior but you can think of it as a prior. IT depends where you are in the process when you're thinking of it as a prior.\n",
    "\n",
    "Q: What's the boundary of the prior at which a pos result will make you believe someone has spleentitis?\n",
    "(Philo Q: so what)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Algorithm\n",
    "\n",
    "For each $h \\in H$,\n",
    "\n",
    "calculate $P(h|D) \\sim P(D|h)P(h)$\n",
    "\n",
    "(Denominator doesn't change maximal h)\n",
    "\n",
    "Output: \n",
    "$h_{map} = argmax_h P(h|D)$\n",
    "\n",
    "MAP = maximum a posterior. Max posterior given all priors.\n",
    "\n",
    "Hard to say what P(h) is.\n",
    "\n",
    "So it's common to drop P(h) and compute\n",
    "\n",
    "$h_{ml} = argmax_h P(D|h)$\n",
    "\n",
    "ML = maximum likelihood hypothesis. Maximum a-priori hypothesis.\n",
    "- Dropping P(h) -> Uniform prior. We're saying our prior hypotheses are equally likely.\n",
    "\n",
    "But **not practical** because we need to look at every h in H.\n",
    "\n",
    "### e.g.s\n",
    "\n",
    "1. Given {<x_i, d_i>} as noise-free examples of c. Binary classification problem.\n",
    "2. c is in H, finite hypothesis class.\n",
    "3. uniform prior (uninformed prior)\n",
    "\n",
    "\n",
    "- $P(h) = \\frac{1}{|H|}$\n",
    "- $P(D|h) = 1$ if $d_i=h(x_i) \\forall x_i, d_i \\in D$, $P(D|h) = 0$ otherwise.\n",
    "- $P(D) = \\sum_{h_i \\in H} P(D|h_i)P(h_i) = \\sum_ {h_i \\in VS_{H,D}} 1 * \\frac{1}{|H|} = \\frac{|VS|}{|H|}$\n",
    "\n",
    "P(D|h) = 1 if H is in the version-space of D.\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
