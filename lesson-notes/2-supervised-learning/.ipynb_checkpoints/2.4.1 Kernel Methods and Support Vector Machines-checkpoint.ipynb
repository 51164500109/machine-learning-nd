{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Support Vector Machines\n",
    "\n",
    "(SVM 1)\n",
    "\n",
    "Drawing it in the middle gives a biggest 'demilitarised' zone.\n",
    "Intuition:\n",
    "* There might bu other minuses near the minunes we can see that we risk chopping off if a line gets too close to the current minuses.\n",
    "* This data is just a sample from the population. // NN algorithms. Lines very close to e.g. the pluses -> believing training data too much. Overfitting.\n",
    "* Middle line is **consistent with the data but commits least to it.**\n",
    "* Interesting because it's not a complex overfit. They're all just lines.\n",
    "\n",
    "Hyperplanes:\n",
    "$$y = w^Tx+b$$\n",
    "* y represents the classification label\n",
    "* w representns parameters for our plane\n",
    "* b moves it out of the origin\n",
    "\n",
    "Taking some new point, projecting it onto the line, looking at the value you get when you project it.\n",
    "\n",
    "Value is positive if you are in the class, negative if you're not.\n",
    "\n",
    "Decision boundary being as far away from the data as possible without being inconsistent with it.\n",
    "\n",
    "Hyperplane equation at the decision boundary (neither positive nor negative output) is $w^Tx + b = 0$. \n",
    "\n",
    "What are the equations of the grey lines?\n",
    "* We know labels are {-1, +1}. Line that brushes up against positive example: want it s.t. the output of the line is +1 on the first point that it encounters.\n",
    "* $w^Tx+b=1$ for top grey line. Similarly, $w^T+b=-1$ for bottom grey line.\n",
    "\n",
    "(img)\n",
    "\n",
    "Need to maximise distance between two grey lines. The lines are parallel to one another. Choose one point on each grey line such that the line between them is perpendicular to the parallel lines.\n",
    "\n",
    "* Point on positive line: $w^Tx_1+b=1$\n",
    "* Point on negative line: $w^Tx_2+b-1$\n",
    "* Subtract to get line $w^T(x_1-x_2)=2\n",
    "* Divide both sides by the length of w: \n",
    "$$\\frac{w_t}{||w||}(x_1-x_2)=\\frac{2}{||w||}$$\n",
    "\n",
    "LHS: $x_1-x_2$ is projected onto the normalised vector (unit length, some direction).  This is callled the **margin**.\n",
    "\n",
    "w represents a vector perpendicular to the line (eqn of a plane)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So we want to maximise $\\frac{2}{||w||}$ while classifying everything correctly. Let's turn the condition into a mathematical expression.\n",
    "\n",
    "That is,\n",
    "$$y_i(w^Tx_i + b) \\geq 1 \\forall i$$.\n",
    "\n",
    "* Q: Why geq 1 as opposed to geq 0?\n",
    "\n",
    "* Solve equivalent problem (LHS):\n",
    "$$\\min \\frac{1}{2}||w||^2$$\n",
    "\n",
    "This is easier because it's a quadratic programming problem and people know how to solvo those in straightforward ways. They always have a unique solution.\n",
    "\n",
    "Transform into quadratic programming form:\n",
    "$$\\max W(\\alpha) = \\sum_i \\alpha_i - \\frac{1}{2}\\sum_{i,j}\\alpha_i\\alpha_j y_iy_jx_i^Tx_j$$\n",
    "s.t. $\\alpha_i \\geq 0, \\sum_i \\alpha_i y_i = 0$.\n",
    "\n",
    "Properties\n",
    "* Once you find $\\alpha$, you can recover w: $w=\\sum_i\\alpha_iy_ix_i$.\n",
    "* You can also recover b from having w.\n",
    "* It turns out that those $\\alpha_i$s are mostly zero. -> Only a few x-s matter. Cause some datapoints don't factor into (don't matter for) w. -> Can find all of support you need in some vectors with the non-zero $\\alpha_i$s. -> **machine that only needs a few support vectors**.\n",
    "* Which vectors matter (will be part of the support vectors)? (Those closer to the line)\n",
    "\n",
    "* Similarities to Nearest Neighbours cause only local points matter. Like KNN except you've already done the work to figure out which ones you need and which ones you can throw away. -> Like instance-based learning but it's not completely lazy. (?)\n",
    "\n",
    "Dot product of $x_i^Tx_j$ -> Length of the projection. Measure of similarity (of direction) -> If they point in opposite directions it'll be a negative, if orthogonal it'll be 0, if in same direction it'll be positive and bigger.\n",
    "* Eqn: Find all pairs of points, figure out which ones matter, and think about how they relate to one another wrt their output labels wrt how similar they are.\n",
    "\n",
    "## Supposing not linearly separable\n",
    "\n",
    "* If have **outlier or intruder**: Can tradeoff: Maximise margin Makes the minimal set of errors while maximising the margin if you were allowed to flip a few points from pos to neg or vv.\n",
    "* 'Linearly married': minuses in a ring around the pluses. **Transform datapoints**.\n",
    "    - e.g. $\\Phi(q) = <q_1^2, q_2^2, \\sqrt2 q_1q_2>$\n",
    "    - $\\Phi(x)^T\\Phi(y) = (x_1y_1+x_2y_2)^2 = (x^T y)^2$ (dot product, circle)\n",
    "    - Different notion of similarity: Now whether or not you fall in a circle vs direction. Distance in different spaces.\n",
    "    - Chose this form but doesn't require that you do this transformation. Can still simply compute the dot product.\n",
    "    - This is the **kernel trick**.\n",
    "    - Turns out for any function that you use, there is some transformation into some higher dimensional space that happens to represent your kernel.\n",
    "    \n",
    "### Kernel Trick\n",
    "- The kernel is the function itself. e.g. $k = (x^Ty)^2$\n",
    "$$\\max W(\\alpha) = \\sum_i \\alpha_i - \\frac{1}{2}\\sum_{i,j}\\alpha_i\\alpha_j y_iy_jk(x_i,x_j)$$\n",
    "\n",
    "\n",
    "Kernel is mech by which we **measure of similarity** , mech by which we **inject domain knowledge** into the SVM algorithm. Just like KNN.\n",
    "\n",
    "And in higher dimensional space, your points are linearly separable.\n",
    "\n",
    "**Common kernels**\n",
    "* Polynomial kernel $k = (x^Ty+c)^p$ -> Like polynomial regression.\n",
    "* $k = e^{\\frac{-||x-y||^2}{2\\sigma^2}}$. If on top of each other, similarity is 1. If very distant, k close to 1. It's symmetric. Like a Gaussian with some width.\n",
    "* $k = tanh(\\betax^Ty + \\theta)$ -> Like a sigmoid.\n",
    "\n",
    "**Good kernels**: Captures your domain knowledge, your notion of similarity.\n",
    "\n",
    "**Requirements: Mercer Condition**: it acts like a distance. Positive semidefinite (well-behaved).\n",
    "- In practice stuff often works even if it doesn't satisfy the Mercer Condition so it's que merciful.\n",
    "\n",
    "#### Applications\n",
    "x, y can be discrete variables as long as you have some notion of similarity than you can define that returns a number. You can think about strings, graphs, images."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Conclusion\n",
    "- Margins and relation to genelatisation and overfitting\n",
    "- Want to max margin\n",
    "- Optimisation problem for finding linear separator that has max margin (quadratic programming)\n",
    "- Support vectors: SVM is as lazy as necessary\n",
    "- Kernel trick (transformations for non-linearly-separable data)\n",
    "\n",
    "General alg q: What are the levers we have for expressing domain knowledge?  "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
