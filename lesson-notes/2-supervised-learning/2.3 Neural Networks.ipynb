{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural Networks\n",
    "\n",
    "* Synapse: Gap between one neutron and another\n",
    "* Info travels down axon and causes synapses excitation to occur on other neurons which can fire by sending out spike trains.\n",
    "* Neurons are computational units.\n",
    "* Neurons are complicated. By first approximation though (by def) they are v simple.\n",
    "\n",
    "(image of artificial \n",
    "\n",
    "## Perceptron\n",
    "1. Inputs x_i: think of them as firing rates or the strength of inputs. \n",
    "2. Multiplied by weights w_i.\n",
    "3. Activation: Sum the w_ix_i. and see if it's >= the firing threshold. If it is, then output 1. Else output 0.\n",
    "4. Output\n",
    "\n",
    "Artificial Neurons can be tuned such that they fire under different things."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Perceptron class\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "    \n",
    "    def activate(self,inputs):\n",
    "        \"\"\"\n",
    "        Takes in @param inputs, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\" \n",
    "\n",
    "        # INSERT YOUR CODE HERE\n",
    "        \n",
    "\n",
    "        # TODO: calculate the strength with which the perceptron fires\n",
    "        activation = np.dot(inputs, self.weights)\n",
    "        \n",
    "        \n",
    "        # TODO: return 0 or 1 based on the threshold\n",
    "        if activation > self.threshold:\n",
    "            result = 1\n",
    "        else:\n",
    "            result = 0\n",
    "            \n",
    "        return result\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    p1 = Perceptron(np.array([1, 2]), 0.)\n",
    "    assert p1.activate(np.array([ 1,-1])) == 0 # < threshold --> 0\n",
    "    assert p1.activate(np.array([-1, 1])) == 1 # > threshold --> 1\n",
    "    assert p1.activate(np.array([ 2,-1])) == 0 # on threshold --> 0\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### What sort of things can ANNs compute?\n",
    "/ How powerful is a perceptron unit?\n",
    "\n",
    "**Perceptrons are always going to compute hyperplanes (lines)**.\n",
    "\n",
    "- Representing the region in an input space that's going to get an output of 0 versus the region that's going to get an output of 1\n",
    "(2D plane)\n",
    "    - Linear programming (x_1 = 0, threshold x_2 = 1.5)\n",
    "\n",
    "Computations expressible as perceptron units\n",
    "- AND (x_1 in {0,1}, x_2 in {0,1}).\n",
    "- OR\n",
    "- NOT (One variable e.g. when x_1 = 0, good. When x_1 = 1, bad.) w_1 = -1, theta = 0.\n",
    "- If we can combine the perceptron functions together, we can represent any boolean function.\n",
    "### Ways \n",
    "- Perceptron rule (threshold)\n",
    "- Gradient descent (unthreshold)\n",
    "\n",
    "### Perceptron rule\n",
    " \n",
    "- Threshold foldled into weights. Add a 1 to the x inputs.\n",
    "- Run while there is error:\n",
    "$$\\Delta w_i = \\eta(y-\\hat y)x_i$$\n",
    "where\n",
    "$$\\hat y = (\\sum_i w_ix_i \\geq 0)$$,\n",
    "\n",
    "$\\hat y$ is boolean and\n",
    "$\\eta$ is the learning rate.\n",
    "\n",
    "If the data is linearly separable, the perceptron rule will find the separation line in finite time.\n",
    "* But often it's not clear if data is linearly separaable, especially if the data has many dimensions."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will update the perceptron class so that it can update\n",
    "# its weights.\n",
    "#\n",
    "# Finish writing the update() method so that it updates the weights according\n",
    "# to the perceptron update rule.\n",
    "# \n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\"\n",
    "               \n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values,self.weights)\n",
    "        \n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        return int(strength > self.threshold)\n",
    "\n",
    "\n",
    "    def update(self, values, train, eta=.1):\n",
    "        \"\"\"\n",
    "        Takes in a 2D array @param values consisting of a LIST of inputs and a\n",
    "        1D array @param train, consisting of a corresponding list of expected\n",
    "        outputs. Updates internal weights according to the perceptron training\n",
    "        rule using these values and an optional learning rate, @param eta.\n",
    "        \"\"\"\n",
    "        \n",
    "        # YOUR CODE HERE\n",
    "        self.weights = self.weights.astype(float)\n",
    "        \n",
    "        # TODO: for each data point...\n",
    "        for i in range(len(train)):\n",
    "            # TODO: obtain the neuron's prediction for that point\n",
    "            prediction = self.activate(values[i])\n",
    "            print(\"prediction for i=\", i, \" : \", prediction)\n",
    "            print(\"train for i=\", i, \" : \", train[i])\n",
    "            # TODO: update self.weights based on prediction accuracy, learning\n",
    "            # rate and input value\n",
    "            for j in range(len(self.weights)):\n",
    "                weight_delta = eta * (train[i] - prediction) * values [i][j]\n",
    "                print(\"weight_delta for j=\", j, \" : \", weight_delta)\n",
    "                self.weights[j] = self.weights[j] + weight_delta\n",
    "                print(\"self.weights after j=\", j, \" is now \", self.weights)\n",
    "            print(\"self.weights after \", i, \" is now \", self.weights)\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    Nothing should show up in the output if all the assertions pass.\n",
    "    \"\"\"\n",
    "    def sum_almost_equal(array1, array2, tol = 1e-6):\n",
    "        return sum(abs(array1 - array2)) < tol\n",
    "\n",
    "    p1 = Perceptron(np.array([1,1,1]),0)\n",
    "    print(\"p1 weights:\", p1.weights)\n",
    "    p1.update(np.array([[2,0,-3]]), np.array([1]))\n",
    "    print(\"p1 weights:\", p1.weights)\n",
    "    print(\"should be equal to np.array([1.2, 1, 0.7])\")\n",
    "    # assert sum_almost_equal(p1.weights, np.array([1.2, 1, 0.7]))\n",
    "\n",
    "    p2 = Perceptron(np.array([1,2,3]),0)\n",
    "    print(\"p2 weights:\", p2.weights)\n",
    "    p2.update(np.array([[3,2,1],[4,0,-1]]),np.array([0,0]))\n",
    "    print(\"p2 weights:\", p2.weights)\n",
    "    print(\"should be equal to np.array([0.7, 1.8, 2.9])\")\n",
    "    # assert sum_almost_equal(p2.weights, np.array([0.7, 1.8, 2.9]))\n",
    "\n",
    "    p3 = Perceptron(np.array([3,0,2]),0)\n",
    "    print(\"p3 weights:\", p3.weights)\n",
    "    p3.update(np.array([[2,-2,4],[-1,-3,2],[0,2,1]]),np.array([0,1,0]))\n",
    "    print(\"p3 weights:\", p3.weights)\n",
    "    print(\"should be equal to np.array([2.7, -0.3, 1.7])\")\n",
    "    # assert sum_almost_equal(p3.weights, np.array([2.7, -0.3, 1.7]))\n",
    "\n",
    "test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Bulding the XOR Network Debugging"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will create a network of perceptrons that can represent\n",
    "# the XOR function, using a network structure like those shown in the previous\n",
    "# quizzes.\n",
    "#\n",
    "# You will need to do two things:\n",
    "# First, create a network o f perceptrons with the correct weights\n",
    "# Second, define a procedure EvalNetwork() which takes in a list of inputs and\n",
    "# outputs the value of this network.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\"\n",
    "               \n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values,self.weights)\n",
    "        \n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        return int(strength > self.threshold)\n",
    "\n",
    "            \n",
    "# Part 1: Set up the perceptron network\n",
    "Network = [\n",
    "    # input layer, declare input layer perceptrons here\n",
    "    [Perceptron(np.array([1.,0.])), Perceptron(np.array([0.5,0.5,])), Perceptron(np.array([0.0,1.0]))], \\\n",
    "    # output node, declare output layer perceptron here\n",
    "    [Perceptron(np.array([1,-2,1]))]\n",
    "]\n",
    "\n",
    "# Part 2: Define a procedure to compute the output of the network, given inputs\n",
    "def EvalNetwork(inputValues, Network):\n",
    "    \"\"\"\n",
    "    Takes in @param inputValues, a list of input values, and @param Network\n",
    "    that specifies a perceptron network. @return the output of the Network for\n",
    "    the given set of inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    x_1 = inputValues[0]\n",
    "    x_2 = inputValues[1]\n",
    "    input = [1,0]\n",
    "    for layer in Network:\n",
    "        output = []\n",
    "        for perceptron in layer:\n",
    "            perceptron_output = perceptron.activate(input)\n",
    "            output.append(perceptron_output)\n",
    "            print \"pw: \", perceptron.weights, \"input: \", input, \"output: \", perceptron_output\n",
    "        output_temp = output\n",
    "        input = output\n",
    "                \n",
    "    \n",
    "    OutputValue = int(output_temp[0])  \n",
    "    # Be sure your output value is a single number\n",
    "    return OutputValue\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    \"\"\"\n",
    "    print \"0 XOR 0 = 0?:\", EvalNetwork(np.array([0,0]), Network)\n",
    "    print \"0 XOR 1 = 1?:\", EvalNetwork(np.array([0,1]), Network)\n",
    "    print \"1 XOR 0 = 1?:\", EvalNetwork(np.array([1,0]), Network)\n",
    "    print \"1 XOR 1 = 0?:\", EvalNetwork(np.array([1,1]), Network)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "\n",
    "Running test()...\n",
    "0 XOR 0 = 0?: pw:  [ 1.  0.] input:  [1, 0] output:  1\n",
    "pw:  [ 0.5  0.5] input:  [1, 0] output:  1\n",
    "pw:  [ 0.  1.] input:  [1, 0] output:  0\n",
    "pw:  [ 1 -2  1] input:  [1, 1, 0] output:  0\n",
    "0\n",
    "0 XOR 1 = 1?: pw:  [ 1.  0.] input:  [1, 0] output:  1\n",
    "pw:  [ 0.5  0.5] input:  [1, 0] output:  1\n",
    "pw:  [ 0.  1.] input:  [1, 0] output:  0\n",
    "pw:  [ 1 -2  1] input:  [1, 1, 0] output:  0\n",
    "0\n",
    "1 XOR 0 = 1?: pw:  [ 1.  0.] input:  [1, 0] output:  1\n",
    "pw:  [ 0.5  0.5] input:  [1, 0] output:  1\n",
    "pw:  [ 0.  1.] input:  [1, 0] output:  0\n",
    "pw:  [ 1 -2  1] input:  [1, 1, 0] output:  0\n",
    "0\n",
    "1 XOR 1 = 0?: pw:  [ 1.  0.] input:  [1, 0] output:  1\n",
    "pw:  [ 0.5  0.5] input:  [1, 0] output:  1\n",
    "pw:  [ 0.  1.] input:  [1, 0] output:  0\n",
    "pw:  [ 1 -2  1] input:  [1, 1, 0] output:  0\n",
    "0\n",
    "All done!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And I figured out I'd got my AND weights wrong. Missed out a threshold=1.0.\n",
    "\n",
    "WOWW I'm such a moron."
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "1 XOR 1 = 0?: pw:  [ 1.  0.] input:  [1 1] output:  1\n",
    "pw:  [ 0.5  0.5] input:  [1 1] output:  0\n",
    "pw:  [ 0.  1.] input:  [1 1] output:  1\n",
    "pw:  [ 1 -2  1] input:  [1, 0, 1] output:  1\n",
    "1\n",
    "All done!\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ah whoops threshold needs to be 0.9999"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# ----------\n",
    "#\n",
    "# In this exercise, you will create a network of perceptrons that can represent\n",
    "# the XOR function, using a network structure like those shown in the previous\n",
    "# quizzes.\n",
    "#\n",
    "# You will need to do two things:\n",
    "# First, create a network o f perceptrons with the correct weights\n",
    "# Second, define a procedure EvalNetwork() which takes in a list of inputs and\n",
    "# outputs the value of this network.\n",
    "#\n",
    "# ----------\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "class Perceptron:\n",
    "    \"\"\"\n",
    "    This class models an artificial neuron with step activation function.\n",
    "    \"\"\"\n",
    "\n",
    "    def __init__(self, weights = np.array([1]), threshold = 0):\n",
    "        \"\"\"\n",
    "        Initialize weights and threshold based on input arguments. Note that no\n",
    "        type-checking is being performed here for simplicity.\n",
    "        \"\"\"\n",
    "        self.weights = weights\n",
    "        self.threshold = threshold\n",
    "\n",
    "\n",
    "    def activate(self, values):\n",
    "        \"\"\"\n",
    "        Takes in @param values, a list of numbers equal to length of weights.\n",
    "        @return the output of a threshold perceptron with given inputs based on\n",
    "        perceptron weights and threshold.\n",
    "        \"\"\"\n",
    "               \n",
    "        # First calculate the strength with which the perceptron fires\n",
    "        strength = np.dot(values,self.weights)\n",
    "        \n",
    "        # Then return 0 or 1 depending on strength compared to threshold  \n",
    "        return int(strength > self.threshold)\n",
    "\n",
    "            \n",
    "# Part 1: Set up the perceptron network\n",
    "Network = [\n",
    "    # input layer, declare input layer perceptrons here\n",
    "    [Perceptron(np.array([1.,0.])), Perceptron(np.array([0.5,0.5,]), threshold=0.99999), Perceptron(np.array([0.0,1.0]))], \\\n",
    "    # output node, declare output layer perceptron here\n",
    "    [Perceptron(np.array([1,-2,1]))]\n",
    "]\n",
    "\n",
    "# Part 2: Define a procedure to compute the output of the network, given inputs\n",
    "def EvalNetwork(inputValues, Network):\n",
    "    \"\"\"\n",
    "    Takes in @param inputValues, a list of input values, and @param Network\n",
    "    that specifies a perceptron network. @return the output of the Network for\n",
    "    the given set of inputs.\n",
    "    \"\"\"\n",
    "    \n",
    "    # YOUR CODE HERE\n",
    "    input = inputValues\n",
    "    for layer in Network:\n",
    "        output = []\n",
    "        for perceptron in layer:\n",
    "            perceptron_output = perceptron.activate(input)\n",
    "            output.append(perceptron_output)\n",
    "            print \"pw: \", perceptron.weights, \"input: \", input, \"output: \", perceptron_output\n",
    "        output_temp = output\n",
    "        input = output\n",
    "                \n",
    "    \n",
    "    OutputValue = int(output_temp[0])  \n",
    "    # Be sure your output value is a single number\n",
    "    return OutputValue\n",
    "\n",
    "\n",
    "def test():\n",
    "    \"\"\"\n",
    "    A few tests to make sure that the perceptron class performs as expected.\n",
    "    \"\"\"\n",
    "    print \"0 XOR 0 = 0?:\", EvalNetwork(np.array([0,0]), Network)\n",
    "    print \"0 XOR 1 = 1?:\", EvalNetwork(np.array([0,1]), Network)\n",
    "    print \"1 XOR 0 = 1?:\", EvalNetwork(np.array([1,0]), Network)\n",
    "    print \"1 XOR 1 = 0?:\", EvalNetwork(np.array([1,1]), Network)\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Gradient Descent\n",
    "- More robust to non(linear separability).\n",
    "Activation\n",
    "$$a = \\sum_i x_i w_i$$\n",
    "\n",
    "Imagine the output is not thresholded. \n",
    "-> figure out weights s.t. not-thresholded value is as close to the output value as we can.\n",
    "\n",
    "$$E(w)=\\frac{1}{2}\\sum_{(x,y)\\in D} (y-a)^2$$\n",
    " \n",
    "Take partial derivative of E(w) with respect to w_i.\n",
    "\n",
    "$$\\frac{\\delta E}{\\delta w_i} = \\sum_{(x,y)\\in 0}(y-a)(-x_i)$$\n",
    "\n",
    "Looks like the perceptron rule.\n",
    "\n",
    "### Comparison of learning rules\n",
    "Perceptron: guarantee of finite convergence in the case of linear separability.\n",
    "$$\\Delta w_i = \\eta(y-\\hat y)x_i$$\n",
    "\n",
    "Gradient descent: calculus. More robust to datasets that are not linearly separable. Converges in the limit to a local optimum.\n",
    "$$\\Delta w_i = \\eta(y-a)x_i$$\n",
    "* Why not do gradient descent on $\\hat y$? -> It's not differentiable because it's discontinuous (a step function).\n",
    "* So we want to try to smooth out the threshold.\n",
    "-> SIGMOID.\n",
    "\n",
    "### Advantages of having threshold vs returning \n",
    "\n",
    "### Tuning perceptron parameters\n",
    "- \n",
    "\n",
    "\n",
    "### Inputs to perceptron networks\n",
    "- A single perceptron is very much like linear regression. Therefore it should take the same kinds of inputs. However the outputs of perceptrons will generally be classifications, not numerical.\n",
    "- A matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variation of Perceptrons\n",
    "\n",
    "I just took the number of perceptrons in each layer and multiplied them together to get the total number of possible outcomes for the quiz. Somehow I think that's wrong.\n",
    "\n",
    "As discussed in the previous lesson, to solve the problem of having only a very few discrete outputs from our neural net, we'll apply a transition function.\n",
    "\n",
    "We'll start by letting you test out a variety of functions, numerically approximating their derivatives in order to apply a gradient descent update rule.\n",
    "\n",
    "We have decided we need a function that is continuous (to avoid the discrete problem of perceptrons) but not linear (to allow us to represent non-linear functions). \n",
    "* Logistic function is appropriate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Sigmoids\n",
    "\n",
    "Sigmoid: S-like.\n",
    "\n",
    "$$ \\sigma(a) = \\frac{1}{1+e^{-a}} $$\n",
    "\n",
    "$ a -> -\\infty$, $\\sigma(a) -> 0$\n",
    "$ a -> +\\infty$, $\\sigma(a) -> 1$\n",
    "\n",
    "$$ D\\sigma(a) = \\sigma(a)(1-\\sigma(a))$$\n",
    "\n",
    "Q: Difference between sigmoid unit and a single perceptron in a binary classification problem?\n",
    "* Sigmoid unit will give more info but both give the same answer.\n",
    "\n",
    "Determine update rules using calculus.\n",
    "\n",
    "### Potential problems with gradient descent\n",
    "(to find locally optimal set of weights)\n",
    "- Local extrema\n",
    "- Lengthy run times\n",
    "- Infinite loops\n",
    "- Failure to completely converge"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Layered networks\n",
    "\n",
    "### Additional layers don't give us more representational power if the units are all linear.\n",
    "\n",
    "(Neural net diagram)\n",
    "\n",
    "If entire neural net is made up of sigmoids, the mapping from input to output is differentiable in terms of the weights.\n",
    "* That is, for any given weight in the network, we can figure out how moving it up or down by a little bit changes the mapping from input to output. So\n",
    "\n",
    "### Back-propagation\n",
    "A computationally beneficial organisation of the chain rule.\n",
    "Info from input _> output\n",
    "error info flowing back from output -> input\n",
    "\n",
    "If we replace the sigmoids with some other differentiable unit, this also works.\n",
    "\n",
    "The error function can have multiple local optima. -> Could just be stuck at an overall non-optimal weight setting.\n",
    "* Imagine combining many parabolas in a higher dimensional space and considering the local minima that are quite high up.\n",
    "\n",
    "### Optimising Weights\n",
    "\n",
    "Methods\n",
    "- Gradient Descent\n",
    "- Advanced methods\n",
    "    - Momentum terms in gradient: instead of being stuck in a high bowl, you can have energy to bounce out and pop over to the text thing.\n",
    "    - Higher order derivatives (look at changes in combinations of weights vs individual weights. e.g. Hamiltonians.)\n",
    "    - Randomised optimisation\n",
    "    - Penalty for 'complexity'. // Decision tree, regression overfitting.\n",
    "        - Networks get complex when we: add more nodes or more layers, have large weights.\n",
    "\n",
    "Some people in ML think optimisation and learning are the same thing."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Restriction Bias\n",
    "What are neural nets appropriate for?\n",
    "\n",
    "Restriction Bias tells you about the representation of the data structure. - Representational power. \n",
    "- Set of hypotheses we're willing to consider.\n",
    "\n",
    "e.g. Perceptrons -> Linear. Half spaces\n",
    "Sigmoids -> More complex.\n",
    "* So not much restruction at all.\n",
    "\n",
    "Types of functions we can represent: \n",
    "* Boolean via network of threshold-like units.\n",
    "* Continuous: Can do this with a single hidden layer as long as we can have as many units as we want. -> Each unit can worry about a small patch of the function. -> Output layer stitch the patches together.\n",
    "* Arbitrary: Adding hidden layers to stitch patches together even if they have jumps between them.\n",
    "    - But that means we have a **danger of overfitting**: We can represent the noise as well. \n",
    "        - Set max number of hidden layers.\n",
    "        - Cross-validation: nodes to put in each layer, number of layers, max weights\n",
    "    - Training NN is an iterative process where error decreases as no. of iterations increase. VS other supervised classification where at some point error increases as no. of iterations increase.\n",
    "    \n",
    "\n",
    "## Preference Bias\n",
    "\n",
    "Algorithm's selection of one representation over another\n",
    "(e.g. DTs correct trees, max information gain)\n",
    "\n",
    "1. How do we start?\n",
    "    - Initial weights: Small, random values. (Randomness gives some variance: If we run multiple times we don't want it to get stuck in the same place multiple times. Small to avoid overfitting.)\n",
    "2. PReference bias\n",
    "    - Prefer correct over incorrect\n",
    "    - Prefer simpler over complex\n",
    "    \n",
    "**Occam's Razor**: Entities should not be multiplied unneccessarily.\n",
    "\n",
    "\n",
    "...Better generalisation error?\n",
    "\n",
    "## Summary\n",
    "- Perceptrons: Linear threshold unit\n",
    "- Networks can be put together to produce any boolean function\n",
    "- Perceptron rule - finite time for linearly separable datasets\n",
    "- General differentiable rule: Back propagation and gradient descent\n",
    "- Preference and restriction bias\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I totally don't get this activation function sandbox quiz"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Josh: ML x design\n",
    "\n",
    "Teaching a computer to recognise sketches\n",
    "* Feature extraction and engineering\n",
    "\n",
    "Applications\n",
    "- Teddy search\n",
    "- Search Doodle 2.0 semantic search horse running annotating motions\n",
    "- Simulate physics \n",
    "- TEDDY 3d mesh\n",
    "- Shadow Draw / Sketch\n",
    "- (Comparing comments)\n",
    "    -> UNderstanding comments (Good or bad)\n",
    "\n",
    "Data: 250 categories, 80 images per category\n",
    "\n",
    "Feature engineering\n",
    "- Word expansion -> Synonyms\n",
    "- Lower case normalise\n",
    "- Bag of words could be two-word groups\n",
    "- Convolution\n",
    "\n",
    "Feautures\n",
    "- Colors RGB\n",
    "- Gradients (hog)\n",
    "- (Feat Engin still) Cluster using KMeans\n",
    "\n",
    "Train: \n",
    "...\n",
    "Test if in 'Top k'\n",
    "\n",
    "? NN extract features for you\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
