{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Decision Making and Reinforcement Learning\n",
    "(RL is a mechanism for doing Decision Making)\n",
    "\n",
    "* Supervised Learning: y = f(x)\n",
    "    * Function approximation\n",
    "    * Given x,y pairs, aim is to find f to map x to y.\n",
    "* Unsupervised Learning: f(x)\n",
    "    * Clustering description\n",
    "    * Given bunch of xs and goal is to find some f that gives a compact description of x.\n",
    "* Reinforcement Learning: y = f(x)\n",
    "    * Given string of x,z pairs of data and learn f that's going to generate ys.\n",
    "    \n",
    "Grid world, 3x4 matrix.\n",
    "- Introduce uncertainty (stochasticity)\n",
    "    - When you choose an action, it executes correctly with prob 0.8\n",
    "    - Moves at a right angle P(0.1), P(0.1).\n",
    "- Q: What is reliability of previous sequence UURRR?\n",
    "\n",
    "Way of capturing these uncertainties directly:\n",
    "# Markov Decision Processes\n",
    "\n",
    "Problem:\n",
    "* States: S\n",
    "    * Set of elements (one for every state you can be in).\n",
    "    * Often have initial and goal states\n",
    "* **Model**: T(s,a,s') ~Pr(s'|s,a)\n",
    "    * Rules of the game you're playing. Physics of the world.   \n",
    "    * T is a function of a state, an action and another state. (That other state s' can be the same as state s.)\n",
    "    * Model is simple in a deterministic world.\n",
    "* **Actions**: A(s), A\n",
    "    * E.g. Up, down, left, right. (No option not to move in this game.)\n",
    "    * Generally we think of it as a function of states.\n",
    "* Reward: R(s), R(s,a), R(s,a,s')\n",
    "    * Scalar value you get for being in a state. E.g. R(goal) = 1, R(red) = -1.\n",
    "    * Reward encompasses our domain knowledge: The usefulness of entering into that state.\n",
    "Solution\n",
    "* Policy: $\\pi(s) -> a$\n",
    "    * Action you should take in a state. Like a command.\n",
    "    * $\\pi^*$ the optimal policy that maximises the long-term expected reward.\n",
    "\n",
    "### Markovian Property\n",
    "1. Only the present matters. You don't have to condition on anything past the most recent state.\n",
    "    - Even if something isn't really Markovian, you can make your state remember everything from the past. -> But that makes it hard to learn cause you'll only ever see each state once\n",
    "    - Could also fold action into state.\n",
    "\n",
    "Another property:\n",
    "2. The model is stationary: The model (rules) don't change. (Definition we use for now)\n",
    "\n",
    "Putting it into contex of RL:\n",
    "* We would like <s,a> pairs to be the training set, with a being the action we SHOULD take.\n",
    "* But what we actually get is <s,a,r> pairs and we need to work out what the optimal policy $\\pi^*$ is. And that's kind of our f.\n",
    "    * s is x\n",
    "   \n",
    "   \n",
    "Policies that are more robust to underlying stochasticities vs plans\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Rewards\n",
    "- Idea of sequences: Actions that set you up for other actions which then lead to rewards.\n",
    "    - Don't know WHAT led to you ending up playing well or badly (reward +1 or -1) -> i.e. what was or what were the action(s) that led to you winning or losing? (Chess analogy) vs SL\n",
    "- **Delayed rewards**\n",
    "- Minor changes matter\n",
    "\n",
    "Temporal Credit Assignment Problem\n",
    "\n",
    "e.g. R(s) = -.04 \n",
    "- (for all states except determined goal state = +1, NO state = -1.)\n",
    "Can represent policy with arrows\n",
    "- End states: Absorbing states\n",
    "(img)\n",
    "- -> **Minor changes (to R(s), say) matter**\n",
    "\n",
    "Bottom right case: Minimise chances of slippage and delay. Encouraged to end the game no matter what vs to end the game on +1 for LHS.\n",
    "\n",
    "Reward // **teaching signal**\n",
    "or because rewards define MDP, rewards are **domain knowledge**.\n",
    "\n",
    "### Sequences of Rewards: Assumptions\n",
    "STATIONARY.\n",
    "\n",
    "1. **Infinite Horizons**\n",
    "    - Assuming you can live forever. E.g. if grid world lasted 10 moves, you might choose to avoid -1 rather than risk -1 to go for +1. (Or you might choose to take more risk.)\n",
    "    - -> Policy can change even if you're in the same state (different number of timesteps left). \n",
    "        - i.e. $\\pi(s,t)$.\n",
    "        - I suppose time could be part of the state.\n",
    "2. **Utility of Sequences** (Addition true based on Stationary Preferences because nothing else can be guaranteed to give this property)\n",
    "    - if $U(S_0, S_1, S_2, ...) >$ $U(S_0, S_1^', S_2^')$\n",
    "then $U(S_1 S2 ...) >$ U(S_1^', S_2^')$\n",
    "    - (Utility over sequence of states)\n",
    "    \n",
    "$$U(S_0 S_1 S_2 ...) = \\sum_{t=0}^\\infty R(s_t)$$\n",
    "\n",
    "- With this rule, infinite accumulation of rewards (1 1 ...) vs (0.5 0.5 ...) no different -> Infty, infty example\n",
    "\n",
    "$$U(S_0 S_1 S_2 ...) = \\sum_{t=0}^\\infty \\gamma^t R(s_t), 0\\leq\\gamma < 1$$\n",
    "$$ \\leq \\sum_{t=0}^\\infty \\gamma^t R_{max} = \\frac{R_{max}}{1-\\gamma}$$\n",
    "\n",
    "Discounted sum. Allows us to go an infinite distance in finite time.\n",
    "\n",
    "**Singularity**: Limit to computer power growing faster is time it takes to design next computer. Computer can design next gen of computers etc. Next gen of computer design its successor twice as fast etc.  Time between generations halves every time. So you will be able to do an infinite number of successors in finite time."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Policies\n",
    "\n",
    "Optimal policy $\\pi^*$ is one that maximises long-term reward\n",
    "$$\\pi^* = \\text{argmax}_\\pi E[\\sum_{t=0}^{\\infty} \\gamma^tR(s_t)|\\pi]$$\n",
    "* Expected value of reward of sequence of states we'll see if we follow pi\n",
    "\n",
    "$$U^{\\pi}(s)=E[\\sum_{t=0}^{\\infty} \\gamma^tR(s_t)|\\pi, s_0=s]$$\n",
    "* How good being in a state given a policy is is exactly what we expect to see from that state on given that policy.\n",
    "* Manages ST-LT tradeoffs. Accounts for late rewards.\n",
    "* $U^{\\pi}(s) \\ne R(s)$\n",
    "\n",
    "$$\\pi^*(s) = \\text{argmax}_a\\sum_{s'}T(s,a,s')U(s')$$\n",
    "where $U(s') = U^{\\pi^*}(s)$\n",
    "\n",
    "Optimal policy maximises expected utility.\n",
    "\n",
    "**Bellman Equation**\n",
    "$$U(s) = R(s) + \\gamma max_a \\sum_{s'} T(s,a,s')U(s')$$\n",
    "* Reward in this state + Discount of all reward you're going to get from the next states\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finding Policies\n",
    "\n",
    "Suppose we have n states. Then we have n Bellman equations\n",
    "$$U(s) = R(s) + \\gamma \\max_a \\sum_{s'} T(s,a,s')U(s')$$\n",
    "\n",
    "and n unknowns U of each s.\n",
    "BUT max makes the equations non-linear. \n",
    "- (Aside you can turn maxes into differentiable stuff that is sometimes useful)\n",
    "\n",
    "**Value Iteration**\n",
    "\n",
    "Algo:\n",
    "- Start with arbitrary utilities\n",
    "- Update utilities based on neighbours\n",
    "    - Neighbours: States they can reach.\n",
    "- Repeat until convergence\n",
    "\n",
    "How to update:\n",
    "- Suppose every time you update is time t.\n",
    "$$\\hat U_{t+1}(s) =  R(s) + \\gamma \\max_a \\sum_{s'} T(s,a,s')\\hat U_t(s')$$\n",
    "- $\\hat U(s')$ is an estimate of utility\n",
    "\n",
    "All n equations are tangled together.\n",
    "\n",
    "\n",
    "* Like a contraction proof. Helps that $\\gamma < 1$.\n",
    "    * R(S) is truth. So 'adding more truth to wrong' so it'll overwhelm the original wrong (initialisation). So $\\hat U_{t+1}(s)$ converges.\n",
    "(Maybe rewatch vid 24 because I was super distracted.)\n",
    "\n",
    "So solving for utility (true value) of a state is the same thing as solving for the optimal policy."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "...\n",
    "(img) (vid 26)\n",
    "\n",
    "...next time utility for x state is greater than 0.\n",
    "So at some point it'll be worth it to try to go up instead of bashing your head against the wall.\n",
    "\n",
    "Value iteration works because eventually value **propagates out** from its neighbours.\n",
    "\n",
    "After more timesteps, you need to figure out the utilities of other states. \n",
    "\n",
    "Policy is a mapping from state to actions, NOT states to utilities. If we have U we can figure out \\pi, but U is more info than we need to figure out \\pi. If U has correct orderings it's sufficient.\n",
    "    - U more like regression, \\pi more like classifier.\n",
    "    \n",
    "\n",
    "#### Policy Iterations (vs value iterations)\n",
    "Emphasis on caring about policies > values.\n",
    "- Start with initial policy $\\pi_0$ <- a guess\n",
    "- Evaluate how good that policy is. Given $\\pi_t$ calculate $U_t =  U^{\\pi}_t$.\n",
    "- Improve: $\\pi_{t+1} = \\text{arg}\\max_a\\sum T(s,a,s')U_t(s')$\n",
    "\n",
    "Allows us to change \\pi over time. E.g. suppose we found a great action in some state. Then all other states that can reach that state might end up taking a different action than they did before because the best action would  be moving towards that state.\n",
    "- How do we calculate U_t? Bellman's equation. $$U_t(s)=R(s)+\\gamma \\sum_{s'} T(s,\\pi_t(s),s')U_t(s')$$\n",
    "    - Instead of max, stick policy in cause we have the policy.\n",
    "    - n equations in n unknowns but there is no max. Now they are **linear equations**.\n",
    "- Fewer iterations than value iteration. Apps.\n",
    "- Bigger jumps than value iterations. Making jumps in policy space rather than in value space.\n",
    "- Computational tricks e.g. do a step of value iteration to get an estimate of $U_t$.\n",
    "- Guaranteed to converge. Finite number of policies and you're always getting better.\n",
    "\n",
    "## Summary\n",
    "- Markov Decision Processes\n",
    "- States, Rewards, Actions, Transitions, (Discounts <- Parameter)\n",
    "    - Capturing the underlying process you care about. Rewards & Discounts capture the nature of the task more than the underlying physics.\n",
    "- Policies\n",
    "- Value functions (Utilities) -> Factor in long-term aspects vs rewards don't.\n",
    "- Discounting: deal with infinite sequences in finite time(?)\n",
    "- Stationary\n",
    "- Bellman equation\n",
    "    - Value iteration\n",
    "    - Policy iteration\n",
    "        - These can be mapped into linear programs and solved in polynomial time.\n",
    "\n",
    "Note: Haven't done any reinforcement learning. in RL you don't necessary know the rewards or transitions. Or indeed the actions or states.\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
