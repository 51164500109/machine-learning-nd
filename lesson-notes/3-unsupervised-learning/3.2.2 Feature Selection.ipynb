{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection\n",
    "Minimal number of features it takes to capture trends in the data.\n",
    "- Select best features\n",
    "- Add new features\n",
    "\n",
    "**Process**\n",
    "- Use human intuition\n",
    "    - POIs send emails to each other at a higher rate\n",
    "- Code up new feature\n",
    "    - Int number of messages to this person from POI\n",
    "- Visualise\n",
    "    - Does the new feature give discriminating power between POIs?\n",
    "- Repeat\n",
    "    - Can we do better? E.g. scale featre by total number of messages to or from that person.\n",
    "\n",
    "Observe\n",
    "- Outliers\n",
    "- Mixture of labelled points: Are there chunks in your visualisation where there are only one category of labels? (e.g. if <20% of emails sent to POIs -> all not POIs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "###\n",
    "### in poiFlagEmail() below, write code that returns a boolean\n",
    "### indicating if a given email is from a POI\n",
    "###\n",
    "\n",
    "import sys\n",
    "import reader\n",
    "import poi_emails\n",
    "\n",
    "def getToFromStrings(f):\n",
    "    '''\n",
    "    The imported reader.py file contains functions that we've created to help\n",
    "    parse e-mails from the corpus. .getAddresses() reads in the opening lines\n",
    "    of an e-mail to find the To: From: and CC: strings, while the\n",
    "    .parseAddresses() line takes each string and extracts the e-mail addresses\n",
    "    as a list.\n",
    "    '''\n",
    "    f.seek(0)\n",
    "    to_string, from_string, cc_string   = reader.getAddresses(f)\n",
    "    to_emails   = reader.parseAddresses( to_string )\n",
    "    from_emails = reader.parseAddresses( from_string )\n",
    "    cc_emails   = reader.parseAddresses( cc_string )\n",
    "\n",
    "    return to_emails, from_emails, cc_emails\n",
    "\n",
    "\n",
    "### POI flag an email\n",
    "\n",
    "def poiFlagEmail(f):\n",
    "    \"\"\" given an email file f,\n",
    "        return a trio of booleans for whether that email is\n",
    "        to, from, or cc'ing a poi \"\"\"\n",
    "\n",
    "    to_emails, from_emails, cc_emails = getToFromStrings(f)\n",
    "\n",
    "    ### poi_emails.poiEmails() returns a list of all POIs' email addresses.\n",
    "    poi_email_list = poi_emails.poiEmails()\n",
    "\n",
    "    to_poi = False\n",
    "    from_poi = False\n",
    "    cc_poi   = False\n",
    "\n",
    "    ### to_poi and cc_poi are related functions, which flag whether\n",
    "    ### the email under inspection is addressed to a POI, or if a POI is in cc\n",
    "    ### you don't have to change this code at all\n",
    "\n",
    "    ### there can be many \"to\" emails, but only one \"from\", so the\n",
    "    ### \"to\" processing needs to be a little more complicated\n",
    "    if to_emails:\n",
    "        ctr = 0\n",
    "        while not to_poi and ctr < len(to_emails):\n",
    "            if to_emails[ctr] in poi_email_list:\n",
    "                to_poi = True\n",
    "            ctr += 1\n",
    "    if cc_emails:\n",
    "        ctr = 0\n",
    "        while not to_poi and ctr < len(cc_emails):\n",
    "            if cc_emails[ctr] in poi_email_list:\n",
    "                cc_poi = True\n",
    "            ctr += 1\n",
    "\n",
    "\n",
    "    #################################\n",
    "    ######## your code below ########\n",
    "    ### set from_poi to True if #####\n",
    "    ### the email is from a POI #####\n",
    "    #################################\n",
    "\n",
    "    if from_emails:\n",
    "        ctr = 0\n",
    "        while not from_poi and ctr < len(from_emails):\n",
    "            if from_emails[ctr] in poi_email_list:\n",
    "                from_poi = True\n",
    "            ctr += 1\n",
    "    \n",
    "    \n",
    "\n",
    "    #################################\n",
    "    return to_poi, from_poi, cc_poi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Beware of bugs - be skeptical of classifiers with near 100% accuracy\n",
    "\n",
    "When Katie was working on the Enron POI identifier, she engineered a feature that identified when a given person was on the same email as a POI. So for example, if Ken Lay and Katie Malone are both recipients of the same email message, then Katie Malone should have her \"shared receipt\" feature incremented. If she shares lots of emails with POIs, maybe she's a POI herself.\n",
    "\n",
    "Here's the problem: there was a subtle bug, that Ken Lay's \"shared receipt\" counter would also be incremented when this happens. And of course, then Ken Lay always shares receipt with a POI, because he is a POI. So the \"shared receipt\" feature became extremely powerful in finding POIs, because it effectively was encoding the label for each person as a feature.\n",
    "\n",
    "We found this first by being suspicious of a classifier that was always returning 100% accuracy. Then we removed features one at a time, and found that this feature was driving all the performance. Then, digging back through the feature code, we found the bug outlined above. We changed the code so that a person's \"shared receipt\" feature was only incremented if there was a different POI who received the email, reran the code, and tried again. The accuracy dropped to a more reasonable level."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting rid of features\n",
    "Reasons\n",
    "- It's noisy\n",
    "- It causes overfitting\n",
    "- It is highly correlated with a feature that's already present\n",
    "- Additional features slow donw training/testing process\n",
    "\n",
    "## Features != Information.\n",
    "Features attempt to access information but are not info themselves. We want the info. // Quantity vs quality.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#!/usr/bin/python\n",
    "\n",
    "import pickle\n",
    "import cPickle\n",
    "import numpy\n",
    "\n",
    "from sklearn import cross_validation\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_selection import SelectPercentile, f_classif\n",
    "\n",
    "\n",
    "\n",
    "def preprocess(words_file = \"../tools/word_data.pkl\", authors_file=\"../tools/email_authors.pkl\"):\n",
    "    \"\"\" \n",
    "        this function takes a pre-made list of email texts (by default word_data.pkl)\n",
    "        and the corresponding authors (by default email_authors.pkl) and performs\n",
    "        a number of preprocessing steps:\n",
    "            -- splits into training/testing sets (10% testing)\n",
    "            -- vectorizes into tfidf matrix\n",
    "            -- selects/keeps most helpful features\n",
    "\n",
    "        after this, the feaures and labels are put into numpy arrays, which play nice with sklearn functions\n",
    "\n",
    "        4 objects are returned:\n",
    "            -- training/testing features\n",
    "            -- training/testing labels\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    ### the words (features) and authors (labels), already largely preprocessed\n",
    "    ### this preprocessing will be repeated in the text learning mini-project\n",
    "    authors_file_handler = open(authors_file, \"r\")\n",
    "    authors = pickle.load(authors_file_handler)\n",
    "    authors_file_handler.close()\n",
    "\n",
    "    words_file_handler = open(words_file, \"r\")\n",
    "    word_data = cPickle.load(words_file_handler)\n",
    "    words_file_handler.close()\n",
    "\n",
    "    ### test_size is the percentage of events assigned to the test set\n",
    "    ### (remainder go into training)\n",
    "    features_train, features_test, labels_train, labels_test = cross_validation.train_test_split(word_data, authors, test_size=0.1, random_state=42)\n",
    "\n",
    "\n",
    "\n",
    "    ### text vectorization--go from strings to lists of numbers\n",
    "    # Some feature selection here  with (1) `stop_words=`english`' and\n",
    "    # (2) max_df -> don't include terms that have a document frequency \n",
    "    # strictly higher than the given thresholdts. \n",
    "    vectorizer = TfidfVectorizer(sublinear_tf=True, max_df=0.5,\n",
    "                                 stop_words='english')\n",
    "    features_train_transformed = vectorizer.fit_transform(features_train)\n",
    "    features_test_transformed  = vectorizer.transform(features_test)\n",
    "\n",
    "\n",
    "\n",
    "    ### feature selection, because text is super high dimensional and \n",
    "    ### can be really computationally chewy as a result\n",
    "    # Select best 10% of features using classifier\n",
    "    selector = SelectPercentile(f_classif, percentile=10)\n",
    "    selector.fit(features_train_transformed, labels_train)\n",
    "    features_train_transformed = selector.transform(features_train_transformed).toarray()\n",
    "    features_test_transformed  = selector.transform(features_test_transformed).toarray()\n",
    "\n",
    "    ### info on the data\n",
    "    print \"no. of Chris training emails:\", sum(labels_train)\n",
    "    print \"no. of Sara training emails:\", len(labels_train)-sum(labels_train)\n",
    "    \n",
    "    return features_train_transformed, features_test_transformed, labels_train, labels_test\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "High dimensionality data -> many features\n",
    "\n",
    "## Bias-Variance Dilemma and Number of Features\n",
    "\n",
    "**High bias**: Pays little attention to data and is oversimplified\n",
    "- e.g. few features used\n",
    "- Low r^2, large SSE\n",
    "**High variance**: Pays too much attention to data, doesn't generalise well. Overfits.\n",
    "- e.g. carefully minimised SSE\n",
    "- Much higher error on test set than on training set\n",
    "\n",
    "Tradeoff between goodness of fit and the simplicity of fit.\n",
    "Want few features, low SSE, high r^2."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Regulatisation: Balancing error with no. of features\n",
    "- Method for automatically penalising extra features in your model\n",
    "Reverse-u plot (quality of model against no. of features)\n",
    "\n",
    "E.g. in regressions\n",
    "\n",
    "### Lasso Regression\n",
    "Minimise SSE + $\\lambda|\\beta|$, \n",
    "\n",
    "where $\\lambda$ is a penalty parameter and\n",
    "$\\beta$ is the coefficients of the regression (related to the number of features used)\n",
    "\n",
    "So gain of feature in minimising SSE has to outweigh the penalty of using that extra feature.\n",
    "\n",
    "$$y = \\sum m_ix_i + b$$\n",
    "\n",
    "**Process: **Lasso regression will try adding features one at a time. If it doesn't decrease SSE sufficiently, it won't add the feature. I.e. it sets the coefficients of those features to zero.\n",
    "\n",
    "Precisely, the **optimisation objective for Lasso is: ** $$(1 / (2 * \\text{n_samples})) * ||y - Xw||^2_2 + \\alpha * ||w||_1$$\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "    from sklearn.linear_model import Lasso\n",
    "features, labels = GetMyData()\n",
    "regression = Lasso()\n",
    "regression.fit(features, labels)\n",
    "regression.predict([2,4])\n",
    "print(\"Coefficients: \", regression.coef_, \"\\nIntercept: \", regression.intercept_)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Feature Selection: Charles & Michael\n",
    "\n",
    "## Why?\n",
    "- Knowledge Discovery, Interpretability and Insight (Human)\n",
    "    - Which features matter\n",
    "- Curse of Dimensionality (Machine)\n",
    "    - The amount of data you need grows exponentially in the number of features you have\n",
    "\n",
    "### How hard is the problem\n",
    "of choosing m features out of n features? (Might not know what m is, m \\leq n.)\n",
    "    - n choose m, or 2^n.\n",
    "    - NP-hard.\n",
    "\n",
    "Two a\n",
    "## Alg approches: Filtering and Wrapping\n",
    "\n",
    "### Filtering:\n",
    "**Process**:\n",
    "- Have input features \n",
    "- Run feat through alg which maximises for some score\n",
    "    - Criteria built in search with no reference to the learner\n",
    "- Passes features to some learning alg which will use it for classification/regression.\n",
    "\n",
    "**Adv**:\n",
    "- Faster: Don't need to worry about paying the cost of what the learner is going to do.\n",
    "- Flow forward\n",
    "\n",
    "**Disadv**:\n",
    "- No feedback. Ignores the learner.\n",
    "- (Speed ->) Tend to look at features is isolation\n",
    "\n",
    "**Examples of criteria**:\n",
    "- Information Gain (depends on labels)\n",
    "    - E.G. Put a decision tree inside the search box. Then the top features that come out of a decision tree go into another learner e.g. KNN. (KNN suffers from Curse of Dim because it doesn't know what features are important.)\n",
    "    - Another version: Neural net and pruning features that have low weight.\n",
    "> Nice\n",
    "- Entropy, Gini index (version of entropy), some form of variance (doesn't depend on the labels)\n",
    "- Linear Independence \n",
    "\n",
    "**Analogies within Supervised Learning**: Decision Trees (**Information Gain**).\n",
    "- Note you can look at labels for filtering in supervised learning.\n",
    "\n",
    "### Wrapping:\n",
    "**Process**:\n",
    "- Take features\n",
    "- Searches over features\n",
    "- Learning alg reports how well it does\n",
    "    - Criteria built in learner\n",
    "- Use that score to search for better set of features\n",
    "\n",
    "**Adv**:\n",
    "- Allows for feedback\n",
    "- Takes into account model bias and the learner\n",
    "\n",
    "**Disadv**:\n",
    "- Much slower.\n",
    "\n",
    "**Examples of criteria**:\n",
    "- Kinds of local search or hill climbing (deterministic gradient search)\n",
    "- Randomised optimisation e.g. mimic or genetic algorithms\n",
    "> Don't know what this is.\n",
    "- Forward sequential selection (Polynomial) ~ Hill climbing where neighbourhood relation is adding one more feature.\n",
    "    - Start with a a feature of your end features.\n",
    "    - Look at your features in isolation.\n",
    "    - Pass first, then second, then third...\n",
    "    - Whichever feature is best you keep.\n",
    "    - Then you look at each of remaining features and add them individually. You pick the best combination.\n",
    "    - etc until the improvement is not significant enough.\n",
    "- Backward elimination \n",
    "    - Hill climbing (Reverse of forward search)\n",
    "- (NOT exhaustive search cause that's exponential)\n",
    "\n",
    "\n",
    "Domain knowledge comes into choice of criteria."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Relevance and Usefulness\n",
    "- What if a feature doesn't provide any information?\n",
    "\n",
    "### Relevance\n",
    "**Relevance ~ Information**\n",
    "- A feature $x_i$ is strongly feature is **strongly relevant** if removing it degrades the **Bayes Optimal Classifier** (on a subset of features).\n",
    "    - Weighted average of all the hypotheses. The best that you could do on average.\n",
    "- $x_i$ is **weakly relevant** if \n",
    "    - not strongly relevant\n",
    "    - There exists a subset of features S such that adding $x_i$ to S improves  BOC.\n",
    "    - e.g. for an AND (a,b), if e = not a, neither a or e is strongly relevant. But they are weakly relevant.\n",
    "- $x_i$ is otherwise irrelevant\n",
    "\n",
    "BOC is the gold standard.\n",
    "\n",
    "### Usefulness\n",
    "Usefulness measures the **effect (of minimising error) on a particular predictor**.\n",
    "- E.g. c = 1 for all features in and AND(a,b) dataset for an origin-constrained perceptron\n",
    "- E.g. relevance is useful wrt the BOC."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary\n",
    "\n",
    "- Feature Selection Definiton\n",
    "- Filtering (Faster? but ignoreos bias) vs Wrapping (Slow but useful)\n",
    "- Relevance (Info) vs usefulness (Reduce error for a particular model)\n",
    "    - Strong and weak relevance\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
