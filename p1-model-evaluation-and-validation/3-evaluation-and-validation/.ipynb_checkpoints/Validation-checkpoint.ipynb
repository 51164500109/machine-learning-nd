{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Validation\n",
    "\n",
    "(Insert Train/Test split etc info from Evaluation Metrics notebook)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Where you use Training vs Testing data\n",
    "\n",
    "1. Train/test split.\n",
    "2. Feature transform e.g. PCA fit then PCA transform.\n",
    "    - PCA fit on training features\n",
    "    - PCA transform on training features\n",
    "    - PCA transform on test features (usually after training SVC) -> Represent test data with principle components found in training data.\n",
    "3. Classifier e.g. SVM fit then SVM predict.\n",
    "    - SVC fit on training features\n",
    "    - SVC predict on test features"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation\n",
    "\n",
    "**Problems with splitting data into training & testing data**:\n",
    "- Want to maximise size of both training and test sets, but there's a tradeoff.\n",
    "\n",
    "### K-fold cross-validation process:\n",
    "1. Partition dataset into k bins.\n",
    "2. Run k separate learning experiments.\n",
    "    - Pick test set\n",
    "    - Train\n",
    "    - Test on testing set\n",
    "3. Average test results from these k experiments.\n",
    "\n",
    "Pick Train/Test or e.g. 10-fold CV based on priorities, which can be\n",
    "- Min training time (train/test)\n",
    "- Min run time (unclear but may as well do CV)\n",
    "- Max accuracy (CV)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'time' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-1-bd23e9b27a8c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[0;32mfrom\u001b[0m \u001b[0msklearn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcross_validation\u001b[0m \u001b[0;32mimport\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      2\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 3\u001b[0;31m \u001b[0mt0\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtime\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      4\u001b[0m \u001b[0mkf\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mKFold\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;36m2\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;31m#length of dataset, no. of folds\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mtrain_indices\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_indices\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mkf\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'time' is not defined"
     ]
    }
   ],
   "source": [
    "from sklearn.cross_validation import KFold\n",
    "\n",
    "t0 = time()\n",
    "kf = KFold(len(data), 2) #length of dataset, no. of folds\n",
    "for train_indices, test_indices in kf:\n",
    "    # Make training and testing datasets\n",
    "    features_train = [word_data[ii] for ii in train_indices]\n",
    "    features_test = [word_data[ii] for ii in test_indices]\n",
    "    authors_train = [authors[ii] for ii in train_indices]\n",
    "    authors_test = [authors[ii] for ii in test_indices]\n",
    "\n",
    "# Debugging\n",
    "print(\"train_indices: \", train_indices)\n",
    "print(\"authors_train: \", authors_train)\n",
    "print(\"authours_test: \"authors_test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note \n",
    "**sklearn k-fold CV just splits data into equal-sized partitions - it doesn't shuffle the data.**\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Cross-Validation for Parameter Tuning\n",
    "\n",
    "### GridSearchCV\n",
    "- Systematically works through multiple combinations of parameter tunes, cross-validating as it goes to determine which tune gives the best performance."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "parameters = {'kernel':('linear', 'rbf'), 'C':[1, 10]}\n",
    "svr = svm.SVC()\n",
    "clf = grid_search.GridSearchCV(svr, parameters)\n",
    "clf.fit(iris.data, iris.target)\n",
    "\n",
    "print(\"Optimal parameter combination found: \", clf.best_params_)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.cross_validation import train_test_split\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on function train_test_split in module sklearn.cross_validation:\n",
      "\n",
      "train_test_split(*arrays, **options)\n",
      "    Split arrays or matrices into random train and test subsets\n",
      "    \n",
      "    Quick utility that wraps input validation and\n",
      "    ``next(iter(ShuffleSplit(n_samples)))`` and application to input\n",
      "    data into a single call for splitting (and optionally subsampling)\n",
      "    data in a oneliner.\n",
      "    \n",
      "    Read more in the :ref:`User Guide <cross_validation>`.\n",
      "    \n",
      "    Parameters\n",
      "    ----------\n",
      "    *arrays : sequence of indexables with same length / shape[0]\n",
      "    \n",
      "        allowed inputs are lists, numpy arrays, scipy-sparse\n",
      "        matrices or pandas dataframes.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            preserves input type instead of always casting to numpy array.\n",
      "    \n",
      "    test_size : float, int, or None (default is None)\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the test split. If\n",
      "        int, represents the absolute number of test samples. If None,\n",
      "        the value is automatically set to the complement of the train size.\n",
      "        If train size is also None, test size is set to 0.25.\n",
      "    \n",
      "    train_size : float, int, or None (default is None)\n",
      "        If float, should be between 0.0 and 1.0 and represent the\n",
      "        proportion of the dataset to include in the train split. If\n",
      "        int, represents the absolute number of train samples. If None,\n",
      "        the value is automatically set to the complement of the test size.\n",
      "    \n",
      "    random_state : int or RandomState\n",
      "        Pseudo-random number generator state used for random sampling.\n",
      "    \n",
      "    stratify : array-like or None (default is None)\n",
      "        If not None, data is split in a stratified fashion, using this as\n",
      "        the labels array.\n",
      "    \n",
      "        .. versionadded:: 0.17\n",
      "           *stratify* splitting\n",
      "    \n",
      "    Returns\n",
      "    -------\n",
      "    splitting : list, length = 2 * len(arrays),\n",
      "        List containing train-test split of inputs.\n",
      "    \n",
      "        .. versionadded:: 0.16\n",
      "            Output type is the same as the input type.\n",
      "    \n",
      "    Examples\n",
      "    --------\n",
      "    >>> import numpy as np\n",
      "    >>> from sklearn.cross_validation import train_test_split\n",
      "    >>> X, y = np.arange(10).reshape((5, 2)), range(5)\n",
      "    >>> X\n",
      "    array([[0, 1],\n",
      "           [2, 3],\n",
      "           [4, 5],\n",
      "           [6, 7],\n",
      "           [8, 9]])\n",
      "    >>> list(y)\n",
      "    [0, 1, 2, 3, 4]\n",
      "    \n",
      "    >>> X_train, X_test, y_train, y_test = train_test_split(\n",
      "    ...     X, y, test_size=0.33, random_state=42)\n",
      "    ...\n",
      "    >>> X_train\n",
      "    array([[4, 5],\n",
      "           [0, 1],\n",
      "           [6, 7]])\n",
      "    >>> y_train\n",
      "    [2, 0, 3]\n",
      "    >>> X_test\n",
      "    array([[2, 3],\n",
      "           [8, 9]])\n",
      "    >>> y_test\n",
      "    [1, 4]\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(train_test_split)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
